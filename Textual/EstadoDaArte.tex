
\chapter{Estado da arte}

\section{\textit{Structure from motion}}

O processo de estimar estruturas 3D partindo de imagens 2D em sequência é o \textit{Structure from motion}. Analogicamente, seria a forma a qual o cérebro processa estruturas 3D partindo de um plano bidimensional, partindo do movimento de objetos.

Tendo como exemplo os planos $C$ e $C'$ na equação \eqref{eq1}, dois pontos, chamados $m$ no plano $C$ e $m'$ no plano $C'$, podem ser chamados de correspondentes se há projeções do mesmo ponto 3D no espaço $M$. Havendo mais de uma imagem, algumas perguntas podem ser feitas, como:

\begin{itemize}
	\item{Dado um ponto $m$ na primeira imagem, onde está seu correspondente $m’$ na segunda imagem?}
	\item{Qual a geometria 3D da cena?}
	\item{Qual a posição relativa de ambas câmeras?}
\end{itemize}
	
Com pelo menos duas imagens em diferentes posições, se torna possível inferir a posição 3D de um ponto utilizando sua diferença de posição em ambas imagens. Para tanto é necessário deduzir a posição de duas retas num sistema de coordenadas em comum e é preciso também ter conhecimento da posição relativa da segunda imagem ou câmera com a primeira, o que pode ser chamado de movimento.

A posição e orientação de uma câmera em relação à origem de um sistema de coordenadas do mundo é dada pela matriz $D$ abaixo:

\begin{equation}
D \; = \; \; 
\begin{bmatrix}
R & t \\
0 & 1
\end{bmatrix}
\end{equation}

\noindent
onde $R$ é uma matriz de rotação 3x3 e t indica uma translação em 3D.

As características de uma câmera são dadas por seus parâmetros instrínsecos, expressos pela matriz $A$ abaixo:

\begin{equation}
A \; = \; \;
\begin{bmatrix}
		f & 0 & u_{0} \\
		0 & f & v_{0} \\
		0 & 0 & 1
\end{bmatrix}
\end{equation}

\noindent
onde $f$ é a distância focal em \textit{pixels}, e ($u_{0}$,$v_{0}$) indicam as coordenadas do ponto principal, também denotado por $c$, referente ao centro da imagem.
A matriz de projeção pode ser calculada a partir de $D$ e $A$, juntamente com a matriz de projeção ortogonal em três dimensões $P_0$ através da equação \eqref{eq1} abaixo:

\begin{equation}\label{eq1}
\begin{bmatrix}
		P
\end{bmatrix}_{3x4}	\; = \; \;
\underbrace{\begin{bmatrix}
		f & 0 & u_{0} \\
		0 & f & v_{0} \\
		0 & 0 & 1
\end{bmatrix}
}_{A}
\underbrace{\begin{bmatrix}
		1&0&0&0 \\
		0&1&0&0 \\
		0&0&1&0 \\
\end{bmatrix}}_{P_{0}}	
\underbrace{\begin{bmatrix}
		R & t \\
		0 & 1		
	\end{bmatrix}}_{D}
\end{equation}
Equação \eqref{eq1}: Cálculo da matriz de projeção $P$ \cite{Faugeras-Geometry}.

Seja $M$ um plano 3D no sistema de coordenadas do mundo, e $m$ e $m'$ projeções de $M$ em duas câmeras direcionadas à $M$, $m$ e $m'$ são dados pelas expressões abaixo:

\begin{equation}\label{eq2}
M \; = \; \;
\begin{bmatrix}
x \\ y \\ z \\ 1
\end{bmatrix}, \; \;
m \; = \; \;
\begin{bmatrix}
u \\ v \\ 1
\end{bmatrix}, \;\;
m' \; = \; \;
\begin{bmatrix}
u' \\ v' \\ 1
\end{bmatrix}
\end{equation}
Equação \eqref{eq2}: $M$, $m$ e $m'$, respectivamente.

Os pontos $m$ e $m'$ estão no sistema de coordenadas das respectivas imagens 2D. A última coordenada, sempre igual a 1, é a coordenada homogênea.
Algebricamente, se soubermos as matrizes de projeção $P$ e $P'$, podemos computar $M$ a partir de $m$ e $m'$, resolvendo o sistema linear de 2 equações, três incógnitas abaixo ($M$, $m$ e $m'$) \cite{Faugeras-Geometry}:

\begin{equation}\label{eq3}
\begin{cases}
	PM = m \\
	P'M = m'
\end{cases}
\end{equation}

Caso $m$ e $m'$ sejam conhecidos, mas se deseja descobrir $P$ e $P'$, é necessário para o sistema ter uma solução que as coordenadas $m$ e $m'$ satisfaçam algumas restrições. Portanto, $m'$ não pode ser um ponto arbitrário na segunda imagem.

Seja um plano $\pi$ contendo pontos 3D, havendo uma homografia entre as imagens e $\pi$, podemos afirmar que há uma homografia entre ambas imagens. Homografia é uma relação projetiva entre os pontos de uma imagem em relação à segunda, onde cada ponto de uma imagem é transformado em seu equivalente na segunda. A matriz de homografia é representada por $M$ na equação \eqref{eq4}, onde $m$ é um ponto na primeira imagem e $m'$ seu equivalente na segunda e a relação de homografia é representada na Figura \ref{fig1}:

\begin{equation}\label{eq4}
m' \simeq Hm.
\end{equation}

Equação \eqref{eq4}: Relação de homografia entre um ponto $m'$ da segunda imagem com um ponto $m$ da primeira, como equação.

\begin{figure}[H]
	\centering
		\includegraphics{Imagens/figura2-1.png}
	\caption{Relação de homografia entre um ponto $m'$ da segunda imagem com um ponto $m$ da primeira. Faugeras et al [2] pag. 22.}
	\label{fig1}
\end{figure}

Essa homografia pode ser utilizada para construir mosaicos, como mostra a Figura \ref{fig2}. Apesar de conseguir gerar um campo de visão maior, não é possível obter coordenadas 3D a partir de imagens com o mesmo centro óptico \cite{Faugeras-Geometry}, pois a ambiguidade continua já que os raios correspondentes são idênticos. Um outro caso que vale frisar é quando os centros ópticos de ambas imagens são o mesmo quando tiradas do mesmo ponto mas apenas com uma rotação, também não será possível haver homografia.

\begin{figure}[H]
	\centering
		\includegraphics[width=1.0\textwidth]{Imagens/figura2-2.png}
	\caption{Duas imagens obtidas com o mesmo ponto de vista, e a imagem resultante da junção das duas com homografia. Faugeras et al \cite{Faugeras-Geometry} pag. 24.}
	\label{fig2}
\end{figure}

A \textbf{Linha Epipolar} é a interseção entre o \textbf{Plano Epipolar} (plano que intercepta ambas imagem e possui a \textit{baseline}, que é a linha que liga ambos centros ópticos) com o plano de cada imagem. O ponto $m$ precisa estar situado nela para que possa ser verificado se há o correspondente $m'$ na segunda imagem. Sabendo da linha epipolar, não é necessário procurar $m'$ em toda a segunda imagem, bastando apenas procurar na linha, reduzindo o espaço 2D para 1D, como mostra a Figura \ref{fig3}.

\begin{figure}[H]
	\centering
		\includegraphics{Imagens/figura2-3.png}
	\caption{Exemplificação da redução de 2D para 1D. Faugeras et al \cite{Faugeras-Geometry} pag. 12.}
	\label{fig3}
\end{figure}

Outra forma de se obter o mesmo resultado é considerar algumas restrições às câmeras ao invés de sua correspondência. Supondo que há uma correspondência válida entre $m$ e $m'$, a posição relativa de ambas as câmeras precisa ser de forma que os raios ópticos $L_{m}$, que cruza o centro óptico da primeira imagem e o ponto $m$, e $L_{m}'$, que cruza o centro óptico da segunda imagem e o ponto $m'$, se interceptem. Algebricamente, já que o ponto $M$ da Figura \ref{fig3} depende de três coordenadas, e a correspondência entre $m$ e $m'$ depende no total de 4 parâmetros, é necessário também que haja uma relação algébrica entre as coordenadas de $m$ e $m'$.

A relação entre o ponto $m$ e a linha epipolar $l'_{m}$ na segunda imagem é linearmente projetiva, já que o raio óptico de $m$ é uma função linear de $m$, e a projeção também é linear. Portanto, há uma matriz 3x3 que descreve essa correspondência, chamada Matriz Fundamental \cite{Faugeras-Geometry}. Dada a linha epipolar do ponto $m: l'_{m} = Fm$, se dois pontos $m$ e $m'$ possuem uma correspondência, o ponto $m'$ pertence à linha epipolar de $m$, satisfazendo a seguinte restrição: 
\begin{equation}\label{eq5}m'^{T}Fm = 0\end{equation} que é bilinear nas coordenadas dos pontos das imagens. Invertendo o papel das duas imagens, $F$ seria transformada em sua versão transposta.

A matriz fundamental depende apenas da configuração das câmeras, seus parâmetros intrínsecos, posição e orientação, e não dos pontos 3D da cena. A Figura \ref{fig4} mostra um exemplo de geometria epipolar:

\begin{figure}[H]
	\centering
		\includegraphics{Imagens/figura2-4.png}
	\caption{Exemplo de geometria epipolar. Dado um ponto $m$, o seu correspondente $m'$ precisa estar na linha epipolar $l'_{m}$. Dada a correspondência válida entre $m$ e $m'$, a interseção dos raios ópticos $L_m$ e $L_m$ é não-vazia, e coplanar com a linha $CC'$. Faugeras et al \cite{Faugeras-Geometry} pag. 25.}
	\label{fig4}
\end{figure}

Normalmente não se assume qualquer relação espacial entre os pontos no espaço, apenas a informação disponível pela correspondência projetiva, ou seja, a correspondência de pontos por projeção linear. A geometria epipolar é a restrição básica que decorre da existência de dois pontos de vista distintos. Ambos podem ser obtidos por duas câmeras diferentes ou com uma câmera em movimento, denominado \textit{Structure from Motion}. As restrições epipolares descrevem totalmente a correspondência de pares de correspondências genéricas entre pontos de cada imagem, e a ambiguidade ao longo da linha epipolar causada pela ambiguidade ao longo dos raios ópticos da operação de projeção. Partindo do fato de que a matriz fundamental depende apenas da geometria da câmera, compila toda a informação disponível das correspondências projetivas. Também a matriz fundamental descreve as restrições epipolares. 

Já que todos os raios ópticos contêm o centro óptico $C$ da primeira câmera, todas as linhas epipolares contêm a projeção de $C$ na segunda imagem nos pontos onde a primeira câmera é vista pela segunda, chamado epipolo. Como mostra a Figura \ref{fig4}, o fato do epipolo da segunda imagem pertencer às linhas epipolares significa que $e'^{T}Fm = 0$, ou, $F^{T}e' = 0$. Invertendo as duas imagens, $Fe = 0$. Pode-se concluir que $F$ é uma matriz de grau 3 e $det(F) = 0$. Como essa restrição algébrica é satisfeita e é apenas definida por um fator de escala, $F$ depende de sete parâmetros.

Para o cálculo da matriz fundamental, tem-se $m$ e $m'$ correspondentes abaixo:

\begin{equation}\label{eq6}
m = \begin{bmatrix}u\\v\\1\\ \end{bmatrix}, \; \; \;
m' = \begin{bmatrix}u'\\v'\\1\\ \end{bmatrix}
\end{equation}

Equação \eqref{eq6}: Matrizes dos pontos $m$ e $m'$.

e $F$ a matriz fundamental:

\begin{equation}\label{eq7}
F = 
\begin{bmatrix}
	F_{11} &  F_{12} &  F_{13}\\
	F_{21} &  F_{22} &  F_{23}\\
	F_{31} &  F_{32} &  F_{33}
\end{bmatrix}
\end{equation}

Equação \eqref{eq7}: Matriz fundamental $F$.

Logo, a restrição epipolar $m'^{T}Fm = 0$ pode ser representada como $U^{T}f = 0$, onde:

\begin{equation}\label{eq8}
f = [F_{11}, F_{12}, F_{13}, F_{21}, F_{22}, F_{23}, F_{31}, F_{32}, F_{33}]
\end{equation}
\begin{equation}\label{eq9}
U = [uu', vu', u', uv', vv', v', uv,1]^{T}
\end{equation}

Equações \eqref{eq7}, \eqref{eq8}: Representações de $f$ e $U$.

O sistema de equações $U^{T}f = 0$ é linear, homogêneo e possui 9 incógnitas, com 8 pares de pontos correspondentes. Caso seja possível obter 8 casamentos, então é possível, no geral, obter uma solução única para $F$, definida por um fator de escala. Essa abordagem é conhecida como o algoritmo de 8 pontos.

\section{\textit{Simultaneous Localization And Mapping}}

O termo \textit{SLAM} é a sigla para \textit{Simultaneous Localization And Mapping}. O \textit{SLAM} foi criado a partir do problema de construir um mapa de um ambiente desconhecido por um robô móvel enquanto ao mesmo tempo navega pelo ambiente, podendo utilizar informações parciais do mapa caso estejam disponíveis. O \textit{SFM} consiste em realizar a reconstrução com base em estruturas estáticas, já o \textit{SLAM} tem como objetivo realizar a reconstrução partindo do movimento realizado, seja em tempo real de um \textit{stream} ou de um vídeo. O \textit{SLAM} consiste de múltiplas partes: Extração de pontos de referência, associação de dados, estimativa de estados, e atualização de ponto de referência. Há várias formas de resolver cada uma dessas partes menores que podem ser exclusivas de cada implementação\cite{SLAM-Dummies}. Na abordagem padrão para o \textit{SLAM} usa-se sensores a \textit{laser} para estimar a distância entre os pontos de referência e realizar os cálculos com base nesses dados. No caso desse trabalho ao invés de \textit{lasers} e sensores de movimento foram buscados métodos para uso de apenas uma câmera em junção com a visão computacional e \textit{Multiple View Geometry} para que essa implementação possa ser usada em dispositivos móveis. O \textit{LSD-SLAM} é uma biblioteca que oferece essa abordagem usando gradientes de imagens digitais e por ser o método utilizado, será discutido nas próximas seções.



\section{Algoritmos de detecção de características}
  
Mesmo utilizando câmeras estéreo, ainda existem problemas para mapear estruturas a partir do movimento das câmeras, onde normalmente há duas câmeras sendo utilizadas para obter mais informações sobre um mesmo objeto sob diferentes pontos de vista, mas a correspondência entre as imagens obtidas e os objetos 3D precisa ser encontrada. Para isso, são utilizadas características distintas como cantos, possuindo também diferentes gradientes em múltiplas direções, ou vértices, de uma imagem para outra. O algoritmo mais conhecido atualmente é o \textit{SIFT} \cite{SIFT}, onde utiliza a máxima para a pirâmide de diferença de \textit{Gauss} \cite{Gauss} como características. Primeiramente o \textit{SIFT} procura uma direção dominante no gradiente, e para deixá-lo invariante à rotação, rotaciona o descritor para que sua orientação seja compatível.
  
  As características adquiridas ao longo do tempo são usadas para reconstruir suas localizações no espaço 3D e o movimento da câmera. Outra alternativa são as “abordagens diretas”, que tentam obter as informações geométricas sem abstração intermediária como características ou cantos.

O \textit{SURF}\cite{SURF} também é um algoritmo conhecido na tarefa de obter características, mas ao invés de obter as diferenças de \textit{Gauss}, se baseia em determinantes de matrizes Hessianas para localização e escala \cite{Hessian}. E ao invés de calcular histogramas dos gradientes, computa a soma dos componentes destes com seus valores absolutos. Todas as características detectadas de todas as imagens então são combinadas.

O \textit{ORB (Oriented FAST and Rotated BRIEF)} surgiu como uma alternativa ao algoritmo de detecção e descrição de pontos característicos \textit{SIFT} na questão de eficiência em smartphones. O \textit{ORB} também se mostra mais eficiente em dispositivos com menos poder de processamento, bem como não ser patenteado, ao contrário do \textit{SIFT} e \textit{SURF}, podendo acarretar em problemas de propriedade intelectual. O \textit{ORB} toma como base a detecção de pontos característicos do \textit{SIFT} e o descritor do \textit{BRIEF} \cite{ORB-Artigo}, já que ambos são bem eficientes nestas tarefas e possuem baixo custo computacional. Um dos problemas demonstrados por \textit{Rublee et al}\cite{ORB-Artigo} em seu artigo é a falta de invariância com relação à rotação no \textit{BRIEF}.

Como dito anteriormente, o algoritmo \textit{FAST} foi escolhido para detecção de \textit{keypoints} em sistemas em tempo real que procuram um casamento isto é, encontrar a similaridade entre duas imagens, entre características visuais. O algoritmo precisava ser incrementado com a pirâmide de \textit{schemes} para escala\cite{ORB-Artigo} e o filtro de Harris\cite{ORB-Artigo} para rejeitar cantos. Ao contrário do \textit{SIFT} e \textit{SURF}, o \textit{FAST} não provém de um operador de orientação. Como descrito em seu artigo, há várias formas de se descrever a orientação de um \textit{keypoint}. Algumas destas formas envolvem computações de histogramas de gradientes, mas tais métodos são muito ostensivos computacionalmente e no caso do \textit{SURF}, levando à aproximações não muito boas.

Para os descritores, o \textit{BRIEF} é utilizado pela robustez para luz, \textit{blur} e distorção de perspectiva, mas é sensível com a rotação do plano \cite{ORB-Artigo}. O \textit{BRIEF} utiliza testes binários para treinar um conjunto de árvores de classificação \cite{ORB-Artigo}. Essas árvores são normalmente treinadas com 500 ou mais \textit{keypoints}, podem ser utilizadas para retornar a assinatura de um \textit{keypoint arbitrário} \cite{ORB-Artigo}.

Infelizmente as combinações encontradas podem ser errôneas, logo é preciso eliminar falsos positivos. Para isso normalmente se é utilizado o \textit{RANSAC}, algoritmo que remove combinações de pontos \textit{outliers}, isto é, combinações que provavelmente não pertencem ao espaço. O \textit{RANSAC} também é utilizado para resolver o \textit{Location Determination Problem}, que tem como objetivo determinar os pontos no espaço que têm projeção numa imagem com localizações conhecidas \cite{RANSAC}.

As características adquiridas ao longo do tempo são usadas para reconstruir suas localizações no espaço 3D e o movimento da câmera. Outra alternativa são as “abordagens diretas”, que tentam obter as informações geométricas sem abstração intermediária como características ou cantos.

\section{RANSAC}
O \textit{RANSAC} é um método iterativo para estimar parâmetros de modelos matemáticos partindo de um conjunto de informações obtidas que possuem \textit{outliers}. Também é considerado um método determinístico que possui um bom resultado com confiabilidade. Foi apresentado em 1981 por Fischler e Bolles para tentar resolver o problema da determinação da localização. Ele parte do princípio que o conjunto de informações possui \textit{inliners}, os quais são informações que podem ser explicadas pelos parâmetros do modelo, apesar que podem ter sofrido ruído, e os \textit{outliers}, que são informações que não condizem com o modelo. Esses \textit{outliers} podem ser provenientes de ruído, medições errôneas ou uma má interpretação das informações. A figura \ref{fig3:1} possui um exemplo de um conjunto de informações, os pontos, e como o \textit{RANSAC} trataria os \textit{inliers} e \textit{outliers}:

\begin{figure}[H]
	\centering
		\includegraphics{Imagens/figura3-1.jpg}
	\caption{Exemplificação do tratamento do \textit{RANSAC} com os \textit{inliers} e \textit{outliers}. Imagem licenciada pela CC 3.0}
	\label{fig3:1}
\end{figure}

\section{Bibliotecas}
  
\subsection{\textit{FIT3D}}
O \textit{FIT3D} foi desenvolvido por Esteban et al\cite{FIT3D}, com o intuito de ser uma ferramenta única para realizar os 5 passos do processo de reconstrução de uma cena, que são: calibração da câmera, estimação de movimento, otimização, reconstrução e modelagem. Foi criado para \textit{MATLAB}® e é composto de várias funções próprias bem como de pacotes de terceiros como o \textit{SIFT}, utilizadas em seu \textit{pipeline} para reconstruir a cena, composto pelas seguintes fases:
\begin{enumerate}
\item Calibração: Na calibração são os parâmetros intrínsecos que descrevem a câmera. Inclui parâmetros para distorção radial, como mostrado no livro de Faugeras cap 4.5 e 4.6 \cite{Faugeras-Geometry}, distância focal, centro de projeção e parâmetros \textit{skew};
 \item Egomotion (odometria visual): A odometria estima o movimento de uma câmera entre quadros baseada na informação obtida das imagens denominada como parâmetros extrínsecos. No \textit{FIT3D}, apenas quadros consecutivos são considerados para movimento;
 \item Refinamento: Dado que as imagens são obtidas com ruído e este é propagado para a estimação de movimento, é desejável que haja uma otimização para melhorar as estimativas;
 \item Reconstrução: Partindo das posições das câmeras, parâmetros de calibração e as imagens, a reconstrução é o processo de se obter uma representação 3D da cena bem como um conjunto de pontos 3D;
 \item Modelagem: A modelagem converte uma nuvem esparsa de pontos 3D em algo de maior ordem, como um plano, superfície ou modelo.
\end{enumerate}
  
 Para obter a distorção radial, o \textit{FIT3D} utiliza o método de \textit{Zhang} \cite{FIT3D}. Infelizmente, o \textit{FIT3D} não é atualizado desde 2010, acarretando numa documentação e código obsoletos, e diversos bugs foram encontrados durantes os testes realizados, inclusive impedindo o processamento da reconstrução do nosso \textit{dataset}. Outro problema encontrado é a impossibilidade de executar o algoritmo em tempo real.

\subsection{\textit{LSD-SLAM}}

Um dos maiores benefícios do \textit{SLAM} monocular, e simultaneamente o seu maior desafio, vem da sua ambiguidade de escala inerente: A escala do mundo não pode ser estimada e muda com o tempo, sendo uma das maiores fontes de erro. Isso permite a troca entre ambientes de diferentes escalas, como ambientes internos com móveis e ambientes externos. Por outro lado sensores escalonados como câmeras estéreo e câmeras de profundidade fornecem resultados mais confiáveis mas não oferecem a mesma flexibilidade tanto na facilidade de troca entre diferentes escalas quanto no \textit{hardware}, fazendo a abordagem monocular funcionar para câmeras de celular que é um \textit{hardware} mais acessível. 

\subsubsection{Componentes do \textit{LSD-SLAM}}

Segundo \cite{LSD-SLAM-Artigo}, o algoritmo consiste de três componentes principais: \textbf{\textit{tracking}} (rastreamento), \textbf{\textit{depth map estimation}} (estimativa de mapa de profundidade) e \textbf{\textit{map optimization}} (otimização de mapa):

\begin{itemize}
	\item{O componente de \textbf{\textit{tracking}} continuamente rastreia novas imagens de câmera; Isso é, ele estima sua pose de corpo rígido, dado por $\mathbf{\xi} \in se(3)$, com relação ao quadro-chave atual, usando a pose do quadro anterior como inicialização.}
	\item{O componente de \textbf{\textit{depth map estimation}} usa quadros rastreados para refinar ou repor o quadro-chave atual. A profundidade é refinada filtrando, entre várias comparações estéreo de pequeno patamar por \textit{pixel}, em intervalos de regularização espacial. Caso a câmera fique muito distante do quadro-chave atual, um novo é inicializado projetando os pontos de quadros-chave próximos já rastreados nele.}
	\item{Assim que o componente do \textbf{\textit{depth map estimation}} define que a imagem atual não deve ser usada para refinamento, mas como um novo quadro-chave, o anterior é reposto pela nova imagem que se torna um quadro-chave. O mapa de profundidade do quadro chave anterior não será mais refinado e é incorporado ao mapa global pelo componente de \textbf{\textit{map optimization}}. Para detectar fechamentos de \textit{loops} e mudança de escala, uma transformada de similaridade, dada por $\mathbf{\xi} \in sim(3)$, é estimada para \textit{keyframes} próximos já capturados.}
\end{itemize}	