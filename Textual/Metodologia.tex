\chapter{Metodologia}

A metodologia deste trabalho foi dividida em duas partes: A obtenção dos dados da câmera, bem como \textit{keypoints} e \textit{keyframes} partindo da câmera de um \textit{smartphone} utilizando um aplicativo de autoria própria, o \textit{PhotoGuide}; e a reconstrução do ambiente utilizando a ferramenta \textit{LSD-SLAM}.


\section{\textit{PhotoGuide}}

Desenvolvido para a plataforma \textit{Android}®, o \textit{PhotoGuide} \cite{PhotoGuide} teve como objetivo primário a reconstrução de ambientes à partir da câmera do dispositivo que fosse executado da plataforma \textit{Android}. O aplicativo utiliza diversos algoritmos providos pela biblioteca de terceiros \textit{OpenCV} \cite{OpenCV}. O objetivo do \textit{PhotoGuide} é, partindo do \textit{stream} da câmera ou recebendo um conjunto de imagens sequenciais, obter os quadros num intervalo pré-determinado e então enviar pares para serem processados utilizando o \textit{ORB}, retornando os keypoints encontrados depois de filtrados, sendo inseridos num banco de dados no final do processo.  Primeiramente foi necessário obter os keypoints da cena, e os algoritmos mais utilizados para isso são \textit{SIFT} e \textit{SURF}, mas infelizmente ambos são patenteados, o que nos obrigaria a lidar com a questão das patentes e permissão de uso. O \textit{ORB} e \textit{BRISK} são algoritmos para obter keypoints e seus descritores, e foram desenvolvidos para terem uma performance semelhante ao \textit{SIFT} e\textit{ SURF}, tornando-se assim a melhor escolha já que são de livre uso.

Primeiramente, os \textit{keypoints} são obtidos diretamente da câmera do aparelho, mas a taxa de captura usada foi 10 quadros por segundo, e passados (como imagens \textit{bitmap} e em pares) para o \textit{ORB}; então são processado os \textit{keypoints} e seus descritores, mas infelizmente é possível haver ruído na imagem e, portanto, ocasionando em falsos casamento entre uma imagem e outra. Para remover o máximo possível de falsos positivos, foram utilizadas três técnicas de refinamento, na ordem: \textit{Cross Check}, \textit{Ratio Filtering} e \textit{RANSAC}.

Ao executar o \textit{ORB} e \textit{BRISK}, são obtidos listas de estruturas que possuem informações acerca do keypoint, bem como informações sobre seu correspondente na segunda imagem. O \textit{Cross Check} consiste em rodar o algoritmo duas vezes, uma procurando pontos da primeira imagem na segunda, e outra vez procurando pontos da segunda imagem na primeira. Por fim, percorre ambos, verificando se para cada casamento da primeira lista, as informações batem com o casamento correspondente da segunda lista.

O \textit{Ratio Filtering} percorre a lista resultante, verificando se há mais de um casamento para cada \textit{keypoint}, já que o ideal é que cada \textit{keypoint} tenha apenas um único correspondente na segunda imagem. Ao encontrar mais de uma ocorrência, descarta os pontos mais distantes.

Por fim, a ultima filtragem é realizada com o \textit{RANSAC} \cite{RANSAC}. O \textit{RANSAC} é um método iterativo para estimar parâmetros de modelos matemáticos partindo de um conjunto de informações obtidas que possuem \textit{outliers}. Também é considerado um método determinístico que possui um bom resultado com confiabilidade. Foi apresentado em 1981 por Fischler e Bolles para tentar resolver o problema da determinação da localização. Ele parte do principio que o conjunto de informações possui inliners, os quais são informações que podem ser explicadas pelos parâmetros do modelo, apesar que podem ter sofrido de ruido, e os outliers, que são informações que não condizem com o modelo. Esses \textit{outliers} podem ser provenientes de ruido, medições errôneas ou uma má interpretação das informações. A figura \ref{fig3:1} possui um exemplo de um conjunto de informações, os pontos, e como o \textit{RANSAC} trataria os \textit{inliners} e \textit{outliners}:

\begin{figure}
	\centering
		\includegraphics{Imagens/figura3-1.jpg}
	\caption{Exemplificação do tratamento do \textit{RANSAC} com os \textit{inliners} e \textit{outliners}. Imagem licenciada pela CC 3.0}
	\label{fig3:1}
\end{figure}
  

Por fim, as comparações entre as imagens eram retornadas, e seus \textit{keypoints} armazenados num banco de dados, para futuro processamento, a figura \ref{fig3:2} mostra um exemplo da execução do app:

\begin{figure}
	\centering
		\includegraphics{Imagens/figura3-2E4-4.png}
	\caption{\textit{Keypoints} encontrados e filtrados entre duas imagens.}
	\label{fig3:2}
\end{figure}

 
Foi adicionado ao aplicativo a opção de realizar a calibração da câmera com o auxílio da biblioteca \textit{OpenCV}, já que foi percebido que a distorção radial da câmera estava influenciando os resultados. A calibração da câmera, numa forma prática, consiste em fornecer um \textit{stream} da câmera ou sequência de fotos, contendo um padrão pré-definido. O \textit{OpenCV} fornece funções para calibrar a câmera de um dispositivo, necessitando das imagens capturadas do padrão, dentre as seguintes opções:

\begin{itemize}
	\item{Tabuleiro de xadrez.}
	\item{Círculos alinhados assimetricamente.}
	\item{Círculos alinhados simetricamente.}
\end{itemize}

Exemplificados pelas imagens 3.3, 3.4 e 3.5, respectivamente; e como o \textit{OpenCV} os reconhece, apresentados nas imagens 3.5 para o tabuleiro de xadrez e 3.6 para círculos assimétricos:

Da esquerda para direita, figura 3.3, 3.4 e 3.5: Padrões para a calibração da câmera.


Da esquerda para direita, figura 3.6 e 3.7: Reconhecimento do tabuleiro de xadrez e círculos alinhados assimetricamente.

Ao fim da calibração, o algoritmo retorna a matriz de calibração da câmera, a qual servirá para remover a distorção radial das imagens capturadas posteriormente; vale frisar que a distorção radial será removida somente se  a matriz de calibração tenha sido calculada corretamente. As figuras 3.8 à 3.11 mostram mais exemplos da execução do aplicativo para obter \textit{keypoints} bem como a contra partida sem filtragem:


Da esquerda para direita, figura 3.8 e 3.9: Execuções do \textit{PhotoGuide} e os \textit{keypoints} filtrados da cena.


Da esquerda para direita, figura 3.10 e 3.11: \textit{Keypoints} não filtrados da cena.

É possível perceber a importância de se detectar e remover falsos positivos, ou \textit{outliers}, já que tal quantidade de pontos errôneos afetaria drasticamente o processamento.

\subsection{Dificuldades Encontradas}

Durante o desenvolvimento do \textit{PhotoGuide}, certos problemas causaram grande dificuldade para a continuação de sua implementação. A ausência de documentação, bem como o tamanho pequeno da comunidade de desenvolvimento utilizando \textit{OpenCV} no \textit{Android} dificultam a pesquisa por casos semelhantes ou para a procura de auxílio com o código fonte, bem como a questão de que a maior parte dos resultados encontrados são escritos para a linguagem \textit{C++}, que é mais permissiva do que \textit{Java}®, a linguagem utilizada no desenvolvimento do \textit{PhotoGuide}. Outros problemas provenientes disso são as diferenças entre implementações, já que em \textit{C++}, é possível realizar diversas operações entre os tipos de \textit{Mat},  que é uma estrutura do \textit{OpenCV} para armazenar informações acerca de uma matriz, fato que não se torna possível em Java já que não foram implementados herdando de uma classe em comum. Para certas tarefas, era possível utilizar o \textit{JNI (Java Native Interface)}\cite{JNI}, que permite que código escrito em \textit{Java }realize chamadas para códigos escritos em outras linguagens, e no caso deste trabalho, \textit{C++}. O \textit{JNI} é utilizado principalmente quando se é necessário utilizar uma biblioteca já implementada em outra linguagem, ou quando a complexidade do código na outra linguagem possui uma menor complexidade. No caso do \textit{PhotoGuide}, não foram encontradas melhorias significativas utilizando \textit{JNI}. 

Outro problema encontrado foi o baixo desempenho, já que a captura constante de quadros junto ao processamento de pares se mostrou demasiadamente onerosa para serem executados num \textit{smartphone}. Tendo em vista esses problemas, optamos  apenas que a captura de quadros e vídeo fosse feita utilizando a \textit{PSEye}® e depois de um tempo usando vídeo capturado de \textit{smartphone} e extraído os seus quadros usado um \textit{software} a parte, e o processamento fosse feito numa máquina de maior poder de processamento, como um \textit{desktop} ou \textit{laptop}.

\section{\textit{FIT3D}}
O \textit{FIT3D} foi desenvolvido por Esteban et al\cite{FIT3D}, com o intuito de ser uma ferramenta única para realizar os 5 passos do processo de reconstrução de uma cena, que são: calibração da câmera, estimação de movimento, otimização, reconstrução e modelagem. Foi criado para \textit{MATLAB}® e é composto de várias funções próprias bem como de pacotes de terceiros como o \textit{SIFT}, utilizadas em seu pipeline para reconstruir a cena, composto pelas seguintes fases:
\begin{description}
	\item[Calibração: ]{Processo de obter os parâmetros intrínsecos que descrevem a câmera. Inclui parâmetros para distorção radial, como mostrado no livro de Faugeras cap 4.5 e 4.6 \cite{Faugeras-Geometry}, distância focal, centro de projeção e parâmetros \textit{skew};}
	\item[Egomotion (odometria visual): ]{É a estimativa de movimento de uma câmera entre quadros baseada na informação obtida das imagens denominada como parâmetros extrínsecos. No \textit{FIT3D}, apenas quadros consecutivos são considerados para movimento;}
	\item[Refinamento: ]{Dado que as imagens são obtidas com ruído e este é propagado para a estimação de movimento, é desejável que haja uma otimização para melhorar as estimativas;}
	\item[Reconstrução: ]{Partindo das posições das câmeras, parâmetros de calibração e as imagens, a reconstrução é o processo de se obter uma representação 3D da cena bem como um conjunto de pontos 3D;}
	\item[Modelagem: ]{Processo de converter uma nuvem esparsa de pontos 3D em algo de maior ordem, como um plano, superfície ou modelo.}
\end{description}

Para obter a distorção radial, o \textit{FIT3D} utiliza o método de Zhang \cite{FIT3D}. Infelizmente, o \textit{FIT3D} não é atualizado desde 2010, acarretando numa documentação e código obsoletos, e diversos bugs foram encontrados durantes os testes realizados, inclusive impedindo o processamento da reconstrução do nosso dataset. Outro problema encontrado é a falta de uma forma que seja possível executar o algorítimo ao vivo, ao contrário do \textit{LSD-SLAM} apresentado na próxima seção.


\section{\textit{LSD-SLAM}}

A abordagem seguinte adotada no projeto foi a reconstrução de ambientes utilizando o \textit{LSD-SLAM}. O \textit{LSD-SLAM} é uma ferramenta poderosa porém com pouca documentação, o uso dela se mostra trabalhoso devido ao número grande de parâmetros de configuração possíveis e condições de luz e posição, cada ambiente possui suas próprias configurações otimizadas. A capacidade dessa ferramenta de exportar seus mapas 3D em um tipo de arquivo de fácil leitura faz dela facilmente adaptada para qualquer projeto de navegação 3D e reconhecimento espacial. Aprimoramentos em usabilidade e documentação são desejadas para futuras versões dessa ferramenta caso seus desenvolvedores continuem atualizando. Nessa sessão serão discutidos particularidades de configuração, uso, funcionamento e métodos da utilização dessa ferramenta.

\subsection{Configurações utilizadas}

O \textit{LSD-SLAM} foi testado nas seguintes configurações de máquina, um desktop e um laptop, respectivamente :

\begin{itemize}
	\item{Processador	\textit{AMD FX(tm)-8350 Eight-Core Processor}, 4000 Mhz, 4 Núcleos, 8 Processadores Lógicos}
	\item{8GB de memória \textit{RAM}}
	\item{Placa gráfica \textit{AMD Radeon R2 260x}}
\end{itemize}

e:

\begin{itemize}
	\item{Processador \textit{Intel® Core(™) i3 M370}, 2399 Mhz, 2 Núcleos, 4 Processadores Lógicos}
	\item{4GB de memória RAM}
	\item{Placa gráfica integrada : \textit{Intel® HD Graphics}}
\end{itemize}	

Softwares e periféricos utilizados:

\begin{itemize}
	\item{\textit{Ubuntu} versão 14.04\cite{Ubuntu}}
	\item{Câmera \textit{PSEye®}, modelo para o \textit{console} \textit{PlayStation®} 3.}
	\item{Câmera embutida no \textit{smartphone Motorola® Moto X Play} 32GB em junção com o aplicativo \textit{OpenCamera} para \textit{Android}.}
	\item{\textit{Software VLC} para extrair os quadros dos vídeos capturados pela câmera.}
	\item{\textit{Software ROS Indigo}.\cite{ROS-Tutorial}}
	\item{\textit{Driver} para \textit{webcam} usb\_cam.\cite{Setup-USBCAM}}
\end{itemize}

No começo da utilização do \textit{LSD-SLAM} foi testada a câmera \textit{Logitech® HD Webcam c270}, porém essa câmera não oferecia a capacidade mínima recomendado pelo \textit{LSD-SLAM} para obter resultados decentes, que era ter taxa de quadros de pelo menos 30 \textit{FPS} e \textit{global shutter}, em que todos os \textit{pixels} do quadro são expostos ao mesmo tempo, a \textit{PSEye®} se enquadrava em ambos os requisitos e foi usada no lugar da \textit{Logitech}. No entanto usar a câmera \textit{PSEye®} apesar de permitir um resultado melhor, era uma câmera \textit{USB} que devido a mobilidade da câmera estava limitada ao fio além do próprio tamanho e peso do \textit{notebook} limitava a captura de \textit{datasets} a ser feita usando um \textit{notebook} com a \textit{PSEye®} conectada. Após algum tempo usando a \textit{PSEye®}, os testes foram migrados para a câmera do \textit{smartphone Motorola® Moto X Play}, que tem como diferencial a sua captura de vídeos em mais de 30\textit{FPS} em alta taxa de \textit{bits}. Apesar da câmera não ser \textit{global shutter} e sim \textit{rolling shutter}, em que os \textit{pixels} são expostos sequencialmente em uma direção, que é mais comum entre câmeras \textit{smartphone}, o resultado for satisfatório para o objetivo do trabalho de oferecer o mapeamento 3D do ambiente a partir de um aparelho móvel. A calibração da câmera \textit{smartphone} foi obtida a partir da calibração do \textit{PhotoGuide} e usada como parâmetro para o \textit{LSD-SLAM} usando o método de conversão na sessão 3.2.3.7.

\subsection{Instalação da estação de trabalho}

Apesar dos detalhes dessa etapa não fazerem parte do escopo desse trabalho, algumas informações técnicas podem auxiliar o leitor a experimentar a ferramenta.

Etapas para preparação:

\begin{enumerate}
	\item{Instalação do \textit{Ubuntu}: A instalação do \textit{Ubuntu} deve ser da versão 12 ou 14 obrigatoriamente, a versão usada no trabalho foi a 14.04.}
	\item{Instalação do  \textit{ROS}: O \textit{ROS} vai ser utilizado para a leitura dos quadros da câmera/\textit{dataset} além de ser onde a ferramenta é implementada.\cite{ROS-Tutorial}}
	\item{Instalação do \textit{LSD-SLAM}.\cite{GitHub-LSD-SLAM}}
	\item{Instalação do  \texttt{usb\_cam}: \texttt{usb\_cam} é um driver \textit{ROS} para habilitar câmeras \textit{USB} a serem utilizadas como \textit{stream} de dados para o \textit{ROS} e consequentemente o \textit{LSD-SLAM}, se não há intenção de usar câmeras, como no caso da utilização de um \textit{dataset}, o driver \textit{USB} não é necessário.}
	\item{Calibração da câmera: Será detalhado nas próximas sessões.}
\end{enumerate}

\subsection{Calibração da câmera}

Uma calibração da câmera do \textit{OpenCV} pode ser usada no \textit{LSD-SLAM} no entanto ele vem na sua instalação uma ferramenta de calibração mais intuitiva e confiável por oferecer um retorno ao usuário se as amostras tiradas são o suficiente ou não. Além disso câmeras \textit{USB} não podem ser configuradas usando o \textit{software} \textit{Android} \textit{PhotoGuide}.

\subsubsection{Imprimindo o padrão tabuleiro de xadrez}

Antes de utilizar a câmera deve-se calibrá-la a fim de eliminar a distorção radial que possa ocorrer ao se capturar quadros da mesma forma que no \textit{OpenCV}.
Primeiramente deve-se imprimir um tabuleiro de xadrez \cite{Setup-CalibrateMonocularCamera}, preferencialmente em uma folha ou cartolina A3 ou A4 e fixado em uma superfície rígida e plana. Essa folha não deve: 

\begin{itemize}
	\item{Estar amassada.}
	\item{Coberta com algum material refletor ou fita adesiva em qualquer parte do tabuleiro.}
	\item{Estar com alguma “casa” do tabuleiro cortada por impressão.}
	\item{Estar com tinta esmaecida ou borrada.}
\end{itemize}	

Todos esses fatores interferem no reconhecimento do padrão fazendo com que o resultado da calibração se torne falho. Com o tabuleiro em mãos, tomando cuidado para não cobrir as “casas” do tabuleiro com elas, mova-se para um lugar espaçoso de pelo menos 5 $m^2$, livre de obstruções e bem iluminado.

\subsubsection{Compilando e construindo a ferramenta de calibração}

Começa-se obtendo as dependências necessárias e compilando o driver usando os seguintes comandos respectivamente:

\texttt{\$ rosdep install camera\_calibration}
\texttt{\$ rosmake camera\_calibration}

Tabela 3.1: Comandos para inicializar a instalação do módulo que fará a calibração da câmera.

\subsubsection{Envio de quadross pela câmera}

Ao final da sessão 5, a câmera deve estar configurada e funcionando. Para ter certeza que a câmera está de fato enviando seus dados para o \textit{ROS}, usa-se o seguinte comando:

\texttt{\$ rostopic list}

Tabela 3.2: Comandos para inicializar a instalação do módulo que fará a calibração da câmera.

Esse comando irá listar todos os topics que são os nodos ativos no \textit{ROS}. A saída deve conter esses \textit{topics}:

\texttt{/usb\_cam/camera\_info}
\texttt{/usb\_cam/image\_raw}

Tabela 3.3: Resultado com os \textit{topics}.

Caso a saída não apresente erros, a câmera está pronta para ser calibrada.

\subsubsection{Rodando o nodo de calibração}

Para começar a calibração é necessário carregar os topics com as imagens da câmera que será calibrada usando o seguinte comando:

\texttt{\$ rosrun camera\_calibration cameracalibrator.py --size 9x6 --square 0.108 image:=/usb\_cam/image\_raw camera:=/usb\_cam}

Tabela 3.4: Inicialização do módulo que calibrará a câmera.

Vale frisar a importância de que o parâmetro \texttt{--size} é o tamanho do tabuleiro, no entanto ele conta não as casas e sim as linhas entre as casas:

Figura 3.12: Exemplo de padrão utilizado para a calibração.

Esse padrão por exemplo tem 10x7 casas, no entanto suas linhas são 9x6. O parâmetro \texttt{--size} deve ser nesse caso 9x6.
O parâmetro \texttt{--square} é o tamanho real do lado das casas do tabuleiro que imprimimos em metros. No caso o comando acima indica que cada casa do tabuleiro possui 0,108 metros ou 10,8 centímetros. É recomendado que o usuário que reproduza estes passos use uma régua para medir o seu tabuleiro.
Após executar o comando, a janela de calibração será aberta, como mostra a figura 3.13:

Figura 3.13 - Janela de configuração reconhecendo do tabuleiro

\subsubsection{Movendo o tabuleiro}

Para se obter uma boa calibração é necessário mover o tabuleiro pelo quadro da câmera de modo que:

\begin{itemize}
	\item{O tabuleiro se encontre na esquerda, direita, topo e baixo do campo de vista da câmera.}
	\begin{itemize}
		\item{Barra X - Campo de vista esquerda/direita.}
		\item{Barra Y - Campo de vista topo/baixo.}
		\item{Barra \textit{Size} - Aproximando/Afastando e Rotação da câmera.}
	\end{itemize}
	\item{O tabuleiro preencha todo o campo de vista.}
	\item{O tabuleiro inclinado para a esquerda,direita,topo,baixo (Barra \textit{Skew})}
\end{itemize}

A cada passo segure o tabuleiro até que que apareça na imagem o destaque do padrão.

Figuras 3.14-3.19 - Demonstração de posições para calibração

Ao mover o tabuleiro, podem ser percebidas 4 Barras (X,Y,\textit{Size},\textit{Skew}) na barra lateral que aumentam de tamanho conforme ele vai capturando amostras. Quando o botão \textit{CALIBRATE} estiver iluminado quer dizer que há dados o suficiente para calibrar, ao ser clicado, começará o processo de calibração.
A calibração dura em média um minuto. A janela estará cinza e inativa durante esse tempo, nesse caso a calibração está ocorrendo e o programa não travou, bastando apenas aguardar.

\subsubsection{Resultados da calibração}

Depois de concluída a calibração os resultados dessa calibração podem ser vistos no terminal como essa. Os valores poderão diferir com os apresentados, dependendo da câmera utilizada:


{\setlength{\parindent}{0cm}

\texttt{D = {[}-0.33758562758914146, 0.11161239414304096, -0.00021819272592442094, -3.0291954\\46330518e-05{]}}

 \texttt{K = {[}430.21554970319971, 0.0, 306.6913434743704, 0.0, 430.53169252696676, 227.22480030078816, 0.0, 0.0, 1.0{]}}

\texttt{R = {[}1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0{]}}

\texttt{P = {[}1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0{]}}

 \texttt{\# oST version 5.0 parameters}


 \texttt{{[}image{]}}


 \texttt{width}

 \texttt{640}



 \texttt{height}

 \texttt{480}



 \texttt{{[}narrow\_stereo/left{]}}


 \texttt{camera matrix}

 \texttt{430.215550 0.000000 306.691343}

 \texttt{0.000000 430.531693 227.224800}

 \texttt{0.000000 0.000000 1.000000}


 \texttt{distortion}

 \texttt{-0.337586 0.111612 -0.000218 -0.000030 0.0000}


 \texttt{rectification}

 \texttt{1.000000 0.000000 0.000000}

 \texttt{0.000000 1.000000 0.000000}

 \texttt{0.000000 0.000000 1.000000}


 \texttt{projection}

 \texttt{1.000000 0.000000 0.000000 0.000000}

 \texttt{0.000000 1.000000 0.000000 0.000000}

 \texttt{0.000000 0.000000 1.000000 0.000000}
}

Tabela 3.5: Resultados da calibração impressos no console
 
Os valores destacados serão utilizados em breve. Uma calibração bem sucedida resultará em linhas retas na vida real aparecerem como linhas retas na imagem corrigida. Uma calibração falha normalmente resulta em imagens em branco, irreconhecíveis ou que não preservam as linhas retas.
Após uma calibração bem sucedida pode-se ajustar o deslizador \textit{SCALE} no topo da janela de calibração para mudar o tamanho da imagem retificada. Uma escala de 0.0 significa que a imagem está formatada de modo que os \textit{pixels} pretos criados para a preencher os espaços deixados pelos \textit{pixels} movidos não estejam presentes. A imagem não terá as curvas pretas de correção mas alguns \textit{pixels} da imagem original serão descartados. A escala de 1.0 significa que todos os \textit{pixels} da imagem original são visíveis mas a imagem corrigida tem bordas pretas onde não há \textit{pixels} de entrada da imagem original.
Se a calibração for satisfatória, o botão \textit{COMMIT} deve ser clicado para enviar os parâmetros de calibração para o armazenamento permanente. A \textit{GUI} fechará e a seguinte mensagem será impressa no console \textit{“writing calibration data to...”}.

\subsubsection{Criação do arquivo de calibração}

Do modo que está, o \textit{LSD-SLAM} irá utilizar a calibração salva, no entanto é interessante criar um arquivo de configuração para ser facilmente reutilizado caso se queira usar essa configuração em outra máquina ou salvar a calibração em um local mais seguro. Esse arquivo de calibração também é importante pois o \texttt{dataset\_slam} necessita dele para executar, ou seja, ele não usa a calibração salva pelo calibrador em disco. Na sessão anterior(3.2.3.6) foram destacadas em laranja alguns valores:

{\setlength{\parindent}{0cm}
\texttt{width}

\texttt{640}


\texttt{height}

\texttt{480}


\texttt{camera matrix}

\texttt{430.215550 0.000000 306.691343}

\texttt{0.000000 430.531693 227.224800}

\texttt{0.000000 0.000000 1.000000}


\texttt{distortion}

\texttt{-0.337586 0.111612 -0.000218 -0.000030 0.0000}}



Tabela 3.6: Informações relevantes extraídas da tabela 3.2.

Esses valores serão usados para compor o arquivo. Em um novo documento de texto insira o seguinte modelo:

{\setlength{\parindent}{0cm}
\texttt{fx/width fy/height cx/width cy/height d}

\texttt{in\_width in\_height}

\texttt{"crop" / "full" / "none"}

\texttt{out\_width out\_height}}

Tabela 3.7: Modelo de arquivo de configuração com legenda

Nesse caso deve-se fazer os cálculos substituindo os valores do modelo com os da suas cores respondentes acima, quanto mais precisas as calculadoras usadas melhor. Na terceira linha:

\texttt{crop} - Corta a imagem para o tamanho máximo enquanto inclui apenas \textit{pixels} válidos.
\texttt{full} - Não corta a imagem mas pode incluir \textit{pixels} inválidos.
\texttt{none} - Não realiza a operação de correção de distorção radial

Recomenda-se o uso do valor \texttt{crop}. O resultado final deve ficar assim de acordo com o o seguinte exemplo:

{\setlength{\parindent}{0cm}
\texttt{0.672211796875 0.89694102708333333333333333333333 0.4792052234375 0.473385 -0.337586 0.111612 -0.000218 -0.000030 0.0000}

\texttt{640 480}

\texttt{crop}

\texttt{640 480}}

Tabela 3.8: Exemplo de arquivo de configuração final.

Uma observação é que ao inserir os resultados no arquivo, deve-se certificar que o ponto flutuante é ‘.’(ponto) e não ‘,’(vírgula) pois isso fará com que o arquivo não funcione corretamente. Após isso arquivo deve ser salvo como texttt{nome\_do\_arquivo\_de\_calibração.cfg} e estará pronto para ser usado. Se estiver usando um arquivo de calibração do \textit{OpenCV}, que foi o usado no \textit{PhotoGuide}, a transformação é similar ao resultado da calibração do \textit{LSD-SLAM}. Considerando a seguinte saída da calibração do \textit{OpenCV}:

{\setlength{\parindent}{0cm}
\texttt{<Camera\_Matrix type\_id="opencv-matrix">}

\texttt{<rows>3</rows>}

\texttt{<cols>3</cols>}

\texttt{<dt>d</dt>}

\texttt{<data>}

\texttt{ 6.5746697944293521e+002 0. 3.1950000000000000e+002}

\texttt{ 0. 6.5746697944293521e+002 2.3950000000000000e+002}

\texttt{ 0. 0. 1.}

\texttt{</data></Camera\_Matrix>}

\texttt{<Distortion\_Coefficients type\_id="opencv-matrix">}

\texttt{<rows>5</rows>}

\texttt{<cols>1</cols>}

\texttt{<dt>d</dt>}

\texttt{<data>}

\texttt{ -4.1802327176423804e-001 5.0715244063187526e-001 0. 0. -5.7843597214487474e-001</data></Distortion\_Coefficients>}}
 
Tabela 3.9: Exemplo de arquivo de calibração do \textit{OpenCV}.

Deve ser convertido para um arquivo no formato \textit{.cfg} de modo que:

{\setlength{\parindent}{0cm}\texttt{fx fy cx cy k1 k2 p1 p2}

\texttt{640 480}

\texttt{"crop" / "full" / "none"}

\texttt{640 480}}

Tabela 3.10: Modelo de arquivo de configuração usando a calibração do \textit{OpenCV}.

Lembrando de converter a notação científica, no caso desse exemplo o arquivo final ficará dessa forma:

{\setlength{\parindent}{0cm}
\texttt{0.065746697944293521 0.03195 0.065746697944293521 0.02395 -0.41802327176423804 0.50715244063187526 0 0}

\texttt{640 480}

\texttt{crop}

\texttt{640 480}}

Tabela 3.11: Exemplo de arquivo de configuração final usando a calibração do \textit{OpenCV}.

\subsection{Utilização da ferramenta}

Com a câmera calibrada pode-se agora começar a usar de fato a ferramenta, \textit{LSD-SLAM} é dividido em dois pacotes \textit{ROS}, \texttt{lsd\_slam\_core} e \texttt{lsd\_slam\_viewer}. \texttt{lsd\_slam\_core} contém o sistema \textit{SLAM} completo, enquanto o \texttt{lsd\_slam\_viewer} é opcionalmente usado para a visualização 3D.
Para impulsionar o sistema do \textit{LSD-SLAM} é o suficiente inicializar um primeiro quadro-chave com uma mapa de profundidade aleatório e alta variância. Com movimentos translacionais da câmera o suficiente nos primeiros segundos o algoritmo “trava” em uma certa configuração, e após algumas propagações de quadros-chaves ela converge para a configuração correta de profundidade.

\subsubsection{\texttt{lsd\_slam\_viewer} - Visualizador 3D}

O visualizador além de ser usado para visualizar também pode ser usado para exportar a nuvem de pontos como .ply. Para operar ao vivo, o seguinte comando é executado:

\texttt{rosrun lsd\_slam\_viewer viewer}

Tabela 3.12: Comando para executar o visualizador 3D.

Esse comando irá abrir a tela de \textit{PointCloudViewer}, nela é possível ver em tempo real como está a reconstrução do ambiente. Também é possível gravar e reproduzir a saída gerada pelas trajetórias usando respectivamente os comandos para gravar e reproduzir:

\texttt{rosbag record /lsd\_slam/graph /lsd\_slam/keyframes /lsd\_slam/liveframes -o file\_pc.bag
rosbag play file\_pc.bag}

Tabela 3.13: Comandos sequenciais para gravar e reproduzir a saída do visualizador no formato .bag

Não haverá necessidade de reiniciar o visualizador, ele irá reiniciar automaticamente ao se carregar uma entrada diferente. Alguns atalhos úteis para usar na janela:

\begin{description}
	\item[r :]{Reset, limpa todos os dados mostrados.}
	\item[w :]{Imprime o número de total de pontos, pontos sendo mostrados no momento, \textit{Keyframes} (Quadros-chave) e restrições no console.}
	\item[p :]{Escreve os pontos atualmente mostras como nuvem de pontos para o arquivo: lsd\_slam\_viewer/pc.ply, que pode ser aberto, por exemplo, no \textit{Meshlab}. Use  em combinação com o \textit{sparsityFactor}  para reduzir o número de pontos escritos.}
\end{description}	

\subsubsection{Obtendo o mapa 3D usando o \texttt{live\_slam}}

Para obter o mapa ao vivo, usando uma câmera, o seguinte comando deve ser executado:

\texttt{rosrun lsd\_slam\_core live\_slam /image:=usb\_cam/image\_raw /camera\_info:=usb\_cam}

Tabela 3.14: Comando para executar o \texttt{live\_slam} com calibração salva no \textit{ROS}.
Os parâmetros destacados em vermelhos estão preenchidos com os que o usuário configurou em sua máquina, no caso desse trabalho foi optados pelas nomenclaturas padrões de instalação. Ao se usar esse comando, apenas as dimensões da imagem e a matriz $K$ das mensagens de \texttt{camera\_info} serão usadas, isto é, o vídeo tem que estar corrigido.

Alternativamente, pode-se especificar um arquivo de calibração usando o seguinte comando:

\texttt{rosrun lsd\_slam\_core live\_slam /image:=usb\_cam/image\_raw \_calib:=/caminho/para/seu/arquivodeccalibracao}

Tabela 3.15: Comando para executar o \texttt{live\_slam} com arquivo de calibração externo.

Novamente, o texto em vermelho corresponde ao que foi configurado pelo usuário anteriormente. O texto \texttt{/caminho/para/seu/arquivodeccalibracao} deve ser trocado para o caminho do arquivo de calibração criado.

As telas a seguir mostram exemplos da ferramenta rodando, nota-se que os pontos escritos na nuvem estão invertidos por causa da captura da câmera. 

Figura 3.20 - \textit{DebugWindow DEPTH} do \texttt{live\_slam.}

Figura 3.21 - \textit{PointCloud} referente à figura 3.20.


Figura 3.22 - \textit{DebugWindow DEPTH} do\texttt{ live\_slam}, continuação.

Figura 3.23 - \textit{PointCloud} referente à figura 3.22. continuação.

Figura 3.24 - \textit{DebugWindow DEPTH} do \texttt{live\_slam}, continuação da execução.

Figura 3.25 - \textit{PointCloud} referente à figura 3.24, continuação da reconstrução.

O output do \texttt{lsd\_slam\_core} está sendo transferido para o visualizador na janela \textit{PointCloud Viewer} no formato de nuvem de pontos que podem ser rotacionados e observados em qualquer ângulo. A janela \textit{DebugWindow DEPTH} é a janela que oferece informações sobre a execução do algoritmo incluindo:

\begin{description}
	\item[Map upd :]{A taxa em que os quadros estão sendo computados.}
	\item[Trk :]{A taxa de quadros do \textit{dataset}/câmera.}
	\item[X/Y/Z :]{O terceiro parâmetro não possui identificação mas se encontra no formato X/Y/Z e significa rotação do quadro atual com o quadro original nos 3 eixos.}
	\item[Dens X\% :]{A densidade média dos pontos.}
	\item[Good X\% :]{ A quantidade de pontos pontos bons que podem ser utilizados pelo algoritmo, importante para verificar condições de distorção na imagem da câmera como: iluminação, foco, movimentação, taxa de bits insuficiente na compressão ou similaridade daquele quadro atual com os feitos anteriormente.}
	\item[Scale X\% :]{Escala entre os pontos.}
	\item{Número de pontos utilizados pelo algoritmo.}
	\item{Quadro atual.}
	\item{Numeração de quadro refinador por keyframe.}
\end{description}

Além disso as cores representam o grau de proximidade de cada gradiente do quadro. Tomando o verde como referência, quanto mais vermelho mais perto o gradiente está da câmera em relação aos gradientes verdes, nesse caso eles estão entre o verde e a câmera. Em contrapartida quanto mais azul estiver o gradiente mais longe da camera em relação ao gradiente verde está da câmera. Pontos e gradientes brancos foram reconhecidos pelo detecção de gradiente mas não conseguiu ser reconhecido em escala considerando o \textit{keyframe} atual, normalmente isso ocorre quando há uma diferença de escala inesperada como um objeto que se move ou se a iluminação ou foco da câmera atrapalhar no reconhecimento.

Essas informações são comuns tanto ao \texttt{live\_slam} quanto ao \texttt{dataset\_slam}. O método \texttt{live\_slam} se caracteriza por servir de teste do ambiente para se fazer o dataset. Ele possui suas vantagens e desvantagens:

Vantagens:

\begin{itemize}
	\item{O funcionamento do algoritmo pode ser visto em tempo real.}
	\item{A condição de iluminação pode ser testada usando esse modo mais rapidamente.}
\end{itemize}

Desvantagens:

\begin{itemize}
	\item{É necessário utilizar em um computador portátil para que possa-se mover com a câmera.}
	\item{A mobilidade enquanto se usa esse método é prejudicada.}
	\item{É mais difícil mudar os parâmetros enquanto roda o algoritmo.}
	\item{Baixa reprodutividade.}
	\item{Consumo de recursos do computador alto.}
\end{itemize}	

Também é possível salvar o vídeo capturado usando o parâmetro do \texttt{lsd\_slam\_viewer} \textit{saveAllVideo}, no entanto essa operação é ainda mais custosa, nas máquinas usadas para os testes essa opção não foi possível ser utilizada.

\subsubsection{Obtendo o mapa 3D usando o \texttt{dataset\_slam}}

Para obter o mapa utilizando um \textit{dataset}, execute o seguinte comando:

\texttt{rosrun lsd\_slam\_core dataset\_slam \_files:=/caminho/para/seu/dataset \_hz:=<hz> \_calib:=/caminho/para/seu/arquivodeccalibracao}

Tabela 3.16: Comando para executar o \texttt{dataset\_slam} com um arquivo de calibração externo

Troque \texttt{/caminho/para/seu/dataset} pelo caminho do seu \textit{dataset}, \texttt{<hz>} idealmente deverá ser trocado para 0, o que permite o tracking e mapping sequencial, mas haverá uma queda de performance do que a contrapartida em tempo real. Por fim, troque \texttt{/caminho/para/seu/arquivodeccalibracao} pelo caminho do seu arquivo de configuração resultante na seção 3.2.3.6. Segue abaixo algumas capturas de tela com o comando sendo executado:

Figuras 3.26 e 3.27 - \textit{PointCloud} e \textit{DebugWindow DEPTH} do \texttt{dataset\_slam}.

Figuras 3.28 e 3.29 - \textit{PointCloud} e \textit{DebugWindow DEPTH} do \texttt{dataset\_slam}.

É necessário também, dependendo do \textit{dataset} usado, um refinamento dos parâmetros de visualização e também da forma que o \texttt{lsd\_slam\_core} opera, pois com os parâmetros padrões é possível perceber que a grama em frente ao prédio atrapalhou a captura dos gradientes.

\subsubsection{Parâmetros \texttt{rqt\_reconfigure}}

Uma característica fundamental da ferramenta é a capacidade de modificar alguns parâmetros do algoritmo, esses parâmetros podem ser modificados usando a ferramenta \texttt{rqt\_reconfigure} que vem junto com o pacote do \textit{LSD-SLAM} que pode ser chamado usando o seguinte comando:

\texttt{rosrun rqt\_reconfigure rqt\_reconfigure}

Tabela 3.17 - Comando para inicialização do \texttt{rqt\_reconfigure}

E então será aberta uma janela com vários parâmetros de acordo com quais janelas do \textit{LSD-SLAM} estão abertas, isto é, o \texttt{lsd\_slam\_core} e o \texttt{lsd\_slam\_viewer}. Para cada uma das janelas os parâmetros podem ser modificados em tempo real. A figura 3.30 mostra alguns parâmetros de configuração.

Figura 3.30 - Tela do \texttt{rqt\_reconfigure}

\subsubsubsection{\textit{minUseGrad}}

Esse parâmetro indica a extensão mínima do gradiente para que o algoritmo o reconheça. Em outras palavras, quanto maior esse valor mais restritivo será a captura de gradientes. diminuindo esse parâmetro, gradientes pequenos como texturas irregulares, grama e outros gradientes não uniformes serão reconhecidos. Deve-se analisar cuidadosamente o quanto do ambiente deseja-se capturar e ajustar esse parâmetro de acordo, assim como o exemplo a seguir:


Figura 3.31 - \textit{minUseGrad} no seu valor padrão de 5.0

Figura 3.32 - \textit{minUseGrad} em 25.0

Pelos exemplos dá para perceber que usar o \textit{minUseGrad} no seu valor padrão de 5.0 acrescenta muito ruido devido à grama, além da própria grama ser uma textura que pode ser facilmente confundida gerando erros de relocalização mais à frente na execução. Ao diminuir o parâmetro, a continuidade do gradiente precisa ser mais uniforme para poder ser aceito pelo algoritmo. Não há heurística para a escolha desse valor e ele deve ser testado para cada ambiente. Também vale frisar que aumentar demais esse valor pode cortar alguns gradientes perfeitamente utilizáveis mas que são menos uniformes como por exemplo a passarela até o prédio na imagem.

\subsubsubsection{\textit{KFUsageWeight e KFDistanceWeight}}


Esses parâmetros influenciam na frequência em que novos keyframes são obtidos. O \textit{KFUsageWeight} é em razão da quantidade de quadros usados para refinar, nesse caso um número maior faz com que o algoritmo capture novos keyframes diminuindo a quantidade de refinamento em cada um consequentemente, como no exemplo a seguir:


Figura 3.33 - \textit{PointCloud} e \textit{keyframes} do corredor do 1º andar do DCOMP usando o \textit{KFUsageWeight} padrão de 4.0 e \textit{KFDistWeight} padrão de 3.0

Figura 3.34 - \textit{PointCloud} e \textit{keyframes} do corredor do 1º andar do DCOMP usando o \textit{KFUsageWeight} máximo de 20.0

Figura 3.35 - \textit{PointCloud} e \textit{keyframes} do corredor do 1º andar do DCOMP usando o \textit{KFUsageWeight} máximo de 20.0

Figura 3.36 - \textit{PointCloud} e \textit{keyframes} do corredor do 1º andar do DCOMP usando o \textit{KFUsageWeight} máximo de 20.0

Figura 3.37 - \textit{PointCloud} e \textit{keyframes} do corredor do 1º andar do Dcomp usando o KFUsageWeight máximo de 20.0

Nas janelas do \textit{PointCloud}, as linhas verdes e vermelhas representam \textit{constraints} entre \textit{keyframes} e as pirâmides azuis são \textit{keyframes} e a posição da câmera em relação ao modelo total. Percebe-se que o valor padrão não reproduziu um resultado tão bom quanto o valor máximo, e também dá para ver que há muito mais \textit{constraints} e \textit{keyframes} nas imagem com o valor máximo. Mas apesar de se ter obtido um resultado melhor nesse caso, há uma carga muito maior desse processamento do que pelo valor padrão, fazendo o computador demorar muito mais para executar essa operação devido a quantidade maior de \textit{keyframes} que o algoritmo tem que rastrear e interligar, a ponto dela não ser bem aplicada no \texttt{live\_slam}, e nem em computadores com potência reduzida. Da mesma forma o \textit{KFDistWeight} também modifica a quantidade de \textit{keyframes}, mas ao contrário do \textit{KFUsageWeight}, ele faz a partir da distancia, ou seja, caso o \textit{dataset} possua vários quadros bem próximos fisicamente um do outro e outros quadros mais afastados poderá haver diminuição de \textit{keyframes} no primeiro caso e o aumento de \textit{keyframes} no segundo caso. Isso faz com que o refinamento dos \textit{keyframes} seja mais dependente do \textit{dataset} fazendo com que seja necessário um cuidado maior na hora de movimentar a câmera para não acabar coletando uma quantidade insuficiente de quadros naquela localização. Segue abaixo o mesmo \textit{dataset} usado acima só que com o valor máximo do \textit{KFDistWeight}.

Figura 3.38 - \textit{PointCloud} e \textit{keyframes} do corredor do 1º andar do DCOMP usando o \textit{KFDistWeight} máximo de 20.0

Figura 3.39 - \textit{PointCloud} e \textit{keyframes} do corredor do 1º andar do DCOMP usando o \textit{KFUsageWeight} máximo de 20.0

Como o novo exemplo demonstra, os \textit{keyframes} se comparados aos do valor padrão são mais numerosos, e o resultado geral também foi melhor. O aumento do parâmetro não é tão custoso para máquina como é no \textit{KFUsageWeight}, assim há a possibilidade de usá-lo em computadores com poder de processamento reduzido ou no \texttt{live\_slam}. É importante a experimentação desses parâmetros para se obter o melhor resultado de acordo com o \textit{dataset} utilizado e a máquina utilizada, para concluir essa sessão, segue um exemplo do valor máximo de ambos os parâmetros juntos.

Figura 3.40 - \textit{PointCloud} e \textit{keyframes} do corredor do 1º andar do DCOMP usando o \textit{KFUsageWeight} e \textit{KFDistWeight} máximos de 20.0

Figura 3.41 - \textit{PointCloud} e \textit{keyframes} do corredor do 1º andar do DCOMP usando o \textit{KFUsageWeight} e \textit{KFDistWeight} máximos de 20.0

