\chapter{Estado da arte}

\section{\textit{Structure from motion}}

\textit{Structure from motion} é o processo de estimar estruturas 3D partindo de imagens sequenciais em duas dimensões [15], que pode conter sinais de movimento, em analogia com a biologia, seria o equivalente ao modo  como os olhos humanos e de outros animais conseguem recuperar estruturas 3D de um plano 2D, nesse caso a retina, partindo de uma cena ou objeto em movimento.

Tendo como exemplo os planos C e C’ na imagem 2.1, dois pontos, chamados m no plano C e m’ no plano C', podem ser chamados de correspondentes se há projeções do mesmo ponto 3D no espaço M. Havendo mais de uma imagem, algumas perguntas podem ser feitas, como:

\begin{itemize}
	\item{Dado um ponto m na primeira imagem, onde está seu correspondente m’ na segunda imagem?}
	\item{Qual a geometria 3D da cena?}
	\item{Qual a posição relativa de ambas câmeras?}
\end{itemize}
	
Com pelo menos duas imagens em diferentes posições, se torna possível inferir a posição 3D de um ponto utilizando sua diferença de posição em ambas imagens. Para tanto é necessário deduzir a posição de duas retas num sistema de coordenadas em comum e é preciso também ter conhecimento da posição relativa da segunda imagem ou câmera com a primeira, o que pode ser chamado de movimento.

Algebricamente, é possível descrever as características da câmera em matrizes chamadas\textbf{matrizes de projeção} [2] com as quais, sabendo seus valores, obtém-se um sistema de equações e computa-se o plano M, abaixo:

%Gerar equação

Equação 2.1: Matriz de projeção P, plano M e retas m\textasciicircum T = [u v 1] e m’\textasciicircum T = [u’ v’ 1], respectivamente.

%Gerar equação

Equação 2.2: Sistema de equações para a matriz de projeção P e P’, respectivamente.

Para encontrar a estrutura 3D da cena, é necessário obter as matrizes de projeção \textbf{P e P’}, pois possuem informações acerca da geometria da cena. As equações da figura 2.2 representam o sistema com três incógnitas e para que seja possível uma solução, as coordenadas de \textbf{m e m’} precisam satisfazer algumas restrições, logo, o ponto \textbf{m’} não pode ser um ponto arbitrário na segunda imagem.

Seja um exemplo em que os pontos 3D estejam num plano II, havendo uma homografia[2] entre II e cada uma das imagens, pode-se concluir que há uma homografia entre ambas imagens, chamada \textbf{homografia planar}, definida por uma matriz 3x3 \textbf{M}. Tal homografia é exemplificada como equação pela figura 2.3 e também na figura 2.4:


Equação 2.3: Relação de homografia entre um ponto m’ da segunda imagem com um ponto m da primeira, como equação.

Figura 2.1: Relação de homografia entre um ponto m’ da segunda imagem com um ponto m da primeira. Faugeras et al [2] pag. 22.

Um outro caso que vale frisar é quando os centros ópticos de ambas imagens são o mesmo, como por exemplo, quando tiradas do mesmo ponto mas apenas com uma rotação. Sejam \textbf{m e m’} pontos arbitrários da imagem 1 e 2, respectivamente, para qualquer plano II que não passe pelo centro ótico das imagens, as imagens também são pontos de interseção dos raios em comum de ambas. Portanto, são relacionadas pela matriz \textbf{H}. Tal homografia pode ser utilizada para construir mosaicos, como mostra a figura 2.5 abaixo. Aplicando na primeira imagem, transformando numa imagem maior representando a cena pelo ponto de vista da segunda imagem mas com uma área de visão maior. Apesar de conseguir gerar um campo de visão maior, não é possível obter coordenadas 3D à partir de imagens com o mesmo centro óptico [2], pois a ambiguidade continua já que os raios correspondentes são idênticos.

Figura 2.5: Duas imagens obtidas com o mesmo ponto de vista, e a imagem resultante da junção das duas com homografia. Faugeras et al [2] pag. 24.

Quando os pontos no espaço e as duas câmeras estão em posições distintas, não é possível predizer a posição do correspondente m’ de um ponto qualquer m da primeira imagem, pois tal informação depende da profundidade do ponto 3D ao longo do raio óptico. Geometricamente tal posição não é arbitrária, tal ponto m precisa estar numa linha (o raio óptico), logo m’ precisa estar localizado na projeção da reta na segunda imagem. Tal linha é chamada \textbf{Linha Epipolar}, do ponto \textbf{m} na segunda imagem. Tendo conhecimento dela, ao procurar o correspondente m’, não é necessário procurar em toda a imagem, bastando apenas na linha, reduzindo então uma busca 2D para 1D, mostrado na figura 2.6.


Figura 2.6: Exemplificação da redução de 2D para 1D. Faugeras et al [2] pag. 12.

Outra forma de se obter o mesmo resultado é considerar algumas restrições às câmeras ao invés de sua correspondência. Supondo que há uma correspondência válida entre \textbf{m e m’}, a posição relativa de ambas as câmeras precisa ser de forma que os raios ópticos \textbf{Lm e Lm’} se interceptem. Algebricamente, já que o ponto M da figura 2.5 depende de três coordenadas, e a correspondência entre m e m’ depende no total de 4 parâmetros, é necessário também que haja uma relação algébrica entre as coordenadas de m e m’.

A relação entre o ponto m e a linha epipolar l’m na segunda imagem é linearmente projetiva, já que o raio óptico de m é uma função linear de m, e a projeção também é linear. Portanto, há uma matriz 3x3 que descreve tal correspondência, chamada Matriz Fundamental [2]. Dada a linha epipolar do ponto m: l’m = Fm, se dois pontos m e m’ possuem uma correspondência, o ponto m’ pertence à linha epipolar de m, satisfazendo a seguinte restrição: 
m’TFm = 0, que é bilinear nas coordenadas dos pontos das imagens. Invertendo as duas imagens, F seria transformada em sua versão transposta.

A matriz fundamental depende apenas da configuração das câmeras (seus parâmetros intrínsecos, posição e orientação) e não dos pontos 3D da cena. A figura 2.7 mostra um exemplo de geometria epipolar:

Figura 2.7: Exemplo de geometria epipolar. Dado um ponto m, o seu correspondente m’ precisa estar na linha epipolar l’m. Dada a correspondência válida entre m e m’, a interseção dos raios ópticos Lm e Lm é não-vazia, e coplanar com a linha CC’. Faugeras et al [2] pag. 25.

Normalmente não se assume qualquer relação espacial entre os pontos no espaço, apenas a informação disponível pela correspondência projetiva, ou seja, a correspondência de pontos por projeção linear. A geometria epipolar é a restrição básica que decorre da existência de dois pontos de vista distintos. Ambos podem ser obtidos por duas câmeras diferentes ou com uma câmera em movimento, denominado \textit{Structure from Motion}. As restrições epipolares descrevem totalmente a correspondência de pares de correspondências genéricas entre pontos de cada imagem, e a ambiguidade ao longo da linha epipolar causada pela ambiguidade ao longo dos raios ópticos da operação de projeção. Partindo do fato de que a matriz fundamental depende apenas da geometria da câmera, descreve todas as restrições epipolares, logo compila toda a informação disponível das correspondências projetivas. 

Já que todos os raios ópticos contém o centro óptico C da primeira câmera, todas as linhas epipolares contém a projeção de C na segunda imagem nos pontos onde a primeira câmera é vista pela segunda, chamada epípole. Como mostra a figura 2.5, o fato da epípole  da segunda imagem pertencer às linhas epipolares significa que e’TFm = 0, ou, FTe’ = 0. Invertendo as duas imagens, Fe = 0. Pode-se concluir que F é uma matriz de grau 2 e det(F) = 0. Como essa restrição algébrica é satisfeita e é apenas definida por um fator de escala, F depende de sete parâmetros.

Para o cálculo da matriz fundamental, tem-se m e m’ correspondentes abaixo:


Equação 2.4: Matrizes dos pontos m e m’.

e F a matriz fundamental:

Equação 2.5: Matriz fundamental F.

Logo, a restrição epipolar m’TFm = 0 pode ser representada como UTf = 0, onde:


Equação 2.6: Representações de f, U e UTf.

Tal sistema de equações é linear, homogêneo e possui 9 incógnitas, com 8 pares de pontos correspondentes. É possível encontrar uma solução única para F determinam de até um fator de escala, conhecido também como o algoritmo dos 8 pontos.


\section{Algoritmos de detecção de características}

Encontrar estruturas partindo do movimento apresenta problemas também na visão estéreo, onde há duas câmeras sendo utilizadas para obter mais informações sobre um mesmo objeto sob diferentes pontos de vista, mas a correspondência entre as imagens obtidas e os objetos 3D precisam ser encontradas. Para isso, são utilizadas características distintas como cantos e vértices, possuindo também diferentes gradientes em múltiplas direções, de uma imagem para outra. O algoritmo mais conhecido atualmente é o \textit{SIFT} [16], onde utiliza a máxima para a pirâmide de diferença de Gauss como características. Primeiramente o \textit{SIFT} procura uma direção dominante no gradiente, e para deixá-lo invariante à rotação, rotaciona o descritor para que sua orientação seja compatível.

O \textit{SURF}[20] também é um algoritmo conhecido na tarefa de obter características, mas ao invés de obter as diferenças de Gauss, se baseia em determinantes de matrizes Hessianas para localização e escala [24]. E ao invés de calcular histogramas dos gradientes, computa a soma dos componentes destes com seus valores absolutos Todas as características detectadas de todas as imagens então são combinadas.

O \textit{ORB (Oriented FAST and Rotated BRIEF)} surgiu como uma alternativa ao algoritmo de detecção e descrição de pontos característicos \textit{SIFT} na questão de eficiência em smartphones, já que se mostra mais eficiente em dispositivos com menos poder de processamento, bem como não ser patenteado, ao contrário do \textit{SIFT} e \textit{SURF}, podendo acarretar em problemas de propriedade intelectual. O \textit{ORB} toma como base a detecção de pontos característicos do SIFT e o descritor do BRIEF, já que ambos são bem eficientes nestas tarefas e baixo custo computacional. Um dos problemas demonstrados por Rublee et al[11] em seu artigo é a falta de invariância com relação à rotação no \textit{BRIEF}.

Como dito anteriormente, o algoritmo \textit{FAST} foi escolhido para detecção de \textit{keypoints} em sistemas em tempo real que procuram um casamento isto é, encontrar a similaridade entre duas imagens, entre características visuais, mas o algoritmo precisava ser incrementado com a pirâmide de \textit{schemes} para escala[11] e o filtro de Harris[11] para rejeitar cantos. Ao contrário do \textit{SIFT} e \textit{SURF}, o \textit{FAST} não provém de um operador de orientação. Como descrito em seu artigo, há várias formas de se descrever a orientação de um \textit{keypoint}. Algumas destas formas envolvem computações de histogramas de gradientes, mas tais métodos são muito ostensivos computacionalmente e no caso do \textit{SURF}, levando à aproximações não muito boas.

Para os descritores, o \textit{BRIEF} é utilizado pela robustez para luz, \textit{blur} e distorção de perspectiva, mas é sensível com a rotação do plano [11]. O \textit{BRIEF} utiliza testes binários para treinar um conjunto de árvores de classificação [11], normalmente treinadas com 500 ou mais \textit{keypoints}, podem ser utilizadas para retornar a assinatura de um \textit{keypoint arbitrário} [11].

Infelizmente as combinações encontradas podem ser errôneas, logo é preciso eliminar falsos positivos. Para isso normalmente se é utilizado o \textit{RANSAC} , algoritmo que remove combinações de pontos \textit{outliers}, isto é, combinações que provavelmente não pertencem ao espaço. O \textit{RANSAC} também é utilizado para resolver o \textit{Location Determination Problem}, que tem como objetivo determinar os pontos no espaço que têm projeção numa imagem com localizações conhecidas [19].

As características adquiridas ao longo do tempo são usadas para reconstruir suas localizações no espaço 3D e o movimento da câmera. Outra alternativa são as “abordagens diretas”, que tentam obter as informações geométricas sem abstração intermediária como características ou cantos.


\section{\textit{Simultaneous Localization And Mapping}}

O termo \textit{SLAM} é a sigla para \textit{Simultaneous Localization And Mapping}. O \textit{SLAM} foi criado a partir do problema de construir um mapa de um ambiente desconhecido por um robô móvel enquanto ao mesmo tempo navega pelo ambiente usando o mapa. \textit{SLAM} consiste de múltiplas partes: Extração de pontos de referência, associação de dados, estimativa de estados, e atualização de ponto de referência. Há várias formas de resolver cada uma dessas partes menores que pode ser exclusivo de cada implementação[9]. Na abordagem padrão para o \textit{SLAM} usa-se sensores a laser para estimar a distancia entre os pontos de referência e fazer o cálculos com base nesses dados. No caso desse trabalho ao invés de lasers e sensores de movimento buscou-se métodos para uso de apenas uma câmera em junção com \textit{Computer Vision e Multiple View Geometry} para que essa implementação possa ser usada potencialmente em dispositivos móveis e por pessoas e não robôs. O \textit{LSD-SLAM} oferece essa abordagem usando gradientes de imagens digitais e por ser o método utilizado, será discutido nas próximas sessões.

\section{\textit{LSD-SLAM}}

Um dos maiores benefícios do \textit{SLAM} monocular, e simultaneamente o seu maior desafio, vem da sua ambiguidade de escala inerente: A escala do mundo não pode ser observado e muda com o tempo, sendo uma das maiores fontes de erro. A vantagem é que isso permite a troca entre ambientes de diferentes escalas como ambientes internos com móveis e ambientes externos. Por outro lado sensores escalonados como câmeras estéreo e câmeras de profundidade provém resultados mais confiáveis mas não oferecem a mesma flexibilidade tanto na facilidade de troca entre diferentes escalas quanto no hardware, fazendo a abordagem monocular funcionar para câmeras de celular que é um hardware mais acessível. 

Segundo o artigo do \textit{LSD-SLAM} [12], o algoritmo consiste de três componentes principais: \textbf{tracking (rastreamento)}, \textbf{depth map estimation (estimativa de mapa de profundidade)} e \textbf{map optimization (otimização de mapa)} :

\begin{itemize}
	\item{O componente de \textbf{tracking} continuamente rastreia novas imagens de câmera; Isso é, ele estima sua pose de corpo rígido   se(3) com relação ao quadro-chave atual, usando a pose do quadro anterior como inicialização.}
	\item{O componente de \textbf{depth map estimation} usa quadros rastreados para refinar ou repor o quadro-chave atual. A profundidade é refinada filtrando entre várias comparações estéreo de pequeno patamar por pixel casada com regularização espacial intervalada. Se a câmera se mover muito longe, um novo quadro-chave é inicializado projetando os pontos de quadros-chave perto já rastreados nele.}
	\item{Assim que o componente do \textbf{depth map estimation} define que a imagem atual não deve ser usada para refinamento, mas como um novo quadro-chave, o anterior é reposto pela nova imagem que se torna um quadro-chave. O mapa de profundidade do quadro chave anterior não será mais refinado e é incorporado ao mapa global pelo componente de \textbf{map optimization}. Para detectar fechamentos de \textit{loops} e mudança de escala uma transformada de similaridade   sim(3) para keyframes já capturados próximos é estimada usando um alinhamento direto de imagem sim(3) que é ciente de escala.}
\end{itemize}

\chapter{Metodologia}

A metodologia deste trabalho foi dividida em duas partes: A obtenção dos dados da câmera, bem como \textit{keypoints} e \textit{keyframes} partindo da câmera de um \textit{smartphone} utilizando um aplicativo de autoria própria, o \textit{PhotoGuide}; e a reconstrução do ambiente utilizando a ferramenta \textit{LSD-SLAM}.


\section{\textit{PhotoGuide}}

Desenvolvido para a plataforma \textit{Android}®, o \textit{PhotoGuide} [14] teve como objetivo primário a reconstrução de ambientes à partir da câmera do dispositivo que fosse executado da plataforma \textit{Android}. O aplicativo utiliza diversos algoritmos providos pela biblioteca de terceiros \textit{OpenCV} [17]. O objetivo do \textit{PhotoGuide} é, partindo do \textit{stream} da câmera ou recebendo um conjunto de imagens sequenciais, obter os quadros num intervalo pré-determinado e então enviar pares para serem processados utilizando o \textit{ORB}, retornando os keypoints encontrados depois de filtrados, sendo inseridos num banco de dados no final do processo.  Primeiramente foi necessário obter os keypoints da cena, e os algoritmos mais utilizados para isso são \textit{SIFT} e \textit{SURF}, mas infelizmente ambos são patenteados, o que nos obrigaria a lidar com a questão das patentes e permissão de uso. O \textit{ORB} e \textit{BRISK} são algoritmos para obter keypoints e seus descritores, e foram desenvolvidos para terem uma performance semelhante ao \textit{SIFT} e\textit{ SURF}, tornando-se assim a melhor escolha já que são de livre uso.

Primeiramente, os \textit{keypoints} são obtidos diretamente da câmera do aparelho, mas a taxa de captura usada foi 10 quadros por segundo, e passados (como imagens \textit{bitmap} e em pares) para o \textit{ORB}; então são processado os \textit{keypoints} e seus descritores, mas infelizmente é possível haver ruído na imagem e, portanto, ocasionando em falsos casamento entre uma imagem e outra. Para remover o máximo possível de falsos positivos, foram utilizadas três técnicas de refinamento, na ordem: \textit{Cross Check}, \textit{Ratio Filtering} e \textit{RANSAC}.

Ao executar o \textit{ORB} e \textit{BRISK}, são obtidos listas de estruturas que possuem informações acerca do keypoint, bem como informações sobre seu correspondente na segunda imagem. O \textit{Cross Check} consiste em rodar o algoritmo duas vezes, uma procurando pontos da primeira imagem na segunda, e outra vez procurando pontos da segunda imagem na primeira. Por fim, percorre ambos, verificando se para cada casamento da primeira lista, as informações batem com o casamento correspondente da segunda lista.

O \textit{Ratio Filtering} percorre a lista resultante, verificando se há mais de um casamento para cada \textit{keypoint}, já que o ideal é que cada \textit{keypoint} tenha apenas um único correspondente na segunda imagem. Ao encontrar mais de uma ocorrência, descarta os pontos mais distantes.

Por fim, a ultima filtragem é realizada com o \textit{RANSAC} [19]. O \textit{RANSAC} é um método iterativo para estimar parâmetros de modelos matemáticos partindo de um conjunto de informações obtidas que possuem \textit{outliers}. Também é considerado um método determinístico que possui um bom resultado com confiabilidade. Foi apresentado em 1981 por Fischler e Bolles para tentar resolver o problema da determinação da localização. Ele parte do principio que o conjunto de informações possui inliners, os quais são informações que podem ser explicadas pelos parâmetros do modelo, apesar que podem ter sofrido de ruido, e os outliers, que são informações que não condizem com o modelo. Esses \textit{outliers} podem ser provenientes de ruido, medições errôneas ou uma má interpretação das informações. A figura 3.1 possui um exemplo de um conjunto de informações, os pontos, e como o \textit{RANSAC} trataria os \textit{inliners} e \textit{outliners}:
  
Figura 3.1: Exemplificação do tratamento do RANSAC com os inliners e outliners. Imagem licenciada pela CC 3.0

Por fim, as comparações entre as imagens eram retornadas, e seus keypoints armazenados num banco de dados, para futuro processamento, a figura 3.2 mostra um exemplo da execução do app:


Figura 3.2: Keypoints encontrados e filtrados entre duas imagens.
 
Foi adicionado ao aplicativo a opção de realizar a calibração da câmera com o auxílio da biblioteca OpenCV, já que foi percebido que a distorção radial da câmera estava influenciando os resultados. A calibração da câmera, numa forma prática, consiste em fornecer um stream da câmera ou sequência de fotos, contendo um padrão pré-definido. O OpenCV fornece funções para calibrar a câmera de um dispositivo, necessitando das imagens capturadas do padrão, dentre as seguintes opções:

\begin{itemize}
	\item{Tabuleiro de xadrez;}
	\item{Círculos alinhados assimetricamente e;}
	\item{Círculos alinhados simetricamente.}
\end{itemize}

Exemplificados pelas imagens 3.2, 3.3 e 3.4, respectivamente; e como o \textit{OpenCV} os reconhece, apresentados nas imagens 3.5 para o tabuleiro de xadrez e 3.6 para círculos assimétricos:

Da esquerda para direita, figura 3.3, 3.4 e 3.5: Padrões para a calibração da câmera.


Da esquerda para direita, figura 3.6 e 3.7: Reconhecimento do tabuleiro de xadrez e círculos alinhados assimetricamente.

Ao fim da calibração, o algoritmo retorna a matriz de calibração da câmera, a qual servirá para remover a distorção radial das imagens capturadas posteriormente; vale frisar que a distorção radial será removida somente se  a matriz de calibração tenha sido calculada corretamente. As figuras 3.8 à 3.11 mostram mais exemplos da execução do aplicativo para obter \textit{keypoints} bem como a contra partida sem filtragem:


Da esquerda para direita, figura 3.8 e 3.9: Execuções do \textit{PhotoGuide} e os \textit{keypoints} filtrados da cena.


Da esquerda para direita, figura 3.10 e 3.11: \textit{Keypoints} não filtrados da cena.

É possível perceber a importância de se detectar e remover falsos positivos, ou \textit{outliers}, já que tal quantidade de pontos errôneos afetaria drasticamente o processamento.

\subsection{Dificuldades Encontradas}

Durante o desenvolvimento do \textit{PhotoGuide}, certos problemas causaram grande dificuldade para a continuação de sua implementação. A ausência de documentação, bem como o tamanho pequeno da comunidade de desenvolvimento utilizando \textit{OpenCV} no \textit{Android} dificultam a pesquisa por casos semelhantes ou para a procura de auxílio com o código fonte, bem como a questão de que a maior parte dos resultados encontrados são escritos para a linguagem \textit{C++}, que é mais permissiva do que \textit{Java}®, a linguagem utilizada no desenvolvimento do \textit{PhotoGuide}. Outros problemas provenientes disso são as diferenças entre implementações, já que em \textit{C++}, é possível realizar diversas operações entre os tipos de \textit{Mat},  que é uma estrutura do \textit{OpenCV} para armazenar informações acerca de uma matriz, fato que não se torna possível em Java já que não foram implementados herdando de uma classe em comum. Para certas tarefas, era possível utilizar o \textit{JNI (Java Native Interface)}[22], que permite que código escrito em \textit{Java }realize chamadas para códigos escritos em outras linguagens, e no caso deste trabalho, \textit{C++}. O \textit{JNI} é utilizado principalmente quando se é necessário utilizar uma biblioteca já implementada em outra linguagem, ou quando a complexidade do código na outra linguagem possui uma menor complexidade. No caso do \textit{PhotoGuide}, não foram encontradas melhorias significativas utilizando \textit{JNI}. 

Outro problema encontrado foi o baixo desempenho, já que a captura constante de quadros junto ao processamento de pares se mostrou demasiadamente onerosa para serem executados num \textit{smartphone}. Tendo em vista esses problemas, optamos  apenas que a captura de quadros e vídeo fosse feita utilizando a \textit{PSEye}® e depois de um tempo usando vídeo capturado de \textit{smartphone} e extraído os seus quadros usado um \textit{software} a parte, e o processamento fosse feito numa máquina de maior poder de processamento, como um \textit{desktop} ou \textit{laptop}.

\section{\textit{FIT3D}}
O \textit{FIT3D} foi desenvolvido por Esteban et al[13], com o intuito de ser uma ferramenta única para realizar os 5 passos do processo de reconstrução de uma cena, que são: calibração da câmera, estimação de movimento, otimização, reconstrução e modelagem. Foi criado para \textit{MATLAB}® e é composto de várias funções próprias bem como de pacotes de terceiros como o \textit{SIFT}, utilizadas em seu pipeline para reconstruir a cena, composto pelas seguintes fases:
\begin{description}
	\item[Calibração: ]{Processo de obter os parâmetros intrínsecos que descrevem a câmera. Inclui parâmetros para distorção radial, como mostrado no livro de Faugeras cap 4.5 e 4.6 [2], distância focal, centro de projeção e parâmetros \textit{skew};}
	\item[Egomotion (odometria visual): ]{É a estimativa de movimento de uma câmera entre quadros baseada na informação obtida das imagens denominada como parâmetros extrínsecos. No \textit{FIT3D}, apenas quadros consecutivos são considerados para movimento;}
	\item[Refinamento: ]{Dado que as imagens são obtidas com ruído e este é propagado para a estimação de movimento, é desejável que haja uma otimização para melhorar as estimativas;}
	\item[Reconstrução: ]{Partindo das posições das câmeras, parâmetros de calibração e as imagens, a reconstrução é o processo de se obter uma representação 3D da cena bem como um conjunto de pontos 3D;}
	\item[Modelagem: ]{Processo de converter uma nuvem esparsa de pontos 3D em algo de maior ordem, como um plano, superfície ou modelo.}
\end{description}

Para obter a distorção radial, o \textit{FIT3D} utiliza o método de Zhang [13]. Infelizmente, o \textit{FIT3D} não é atualizado desde 2010, acarretando numa documentação e código obsoletos, e diversos bugs foram encontrados durantes os testes realizados, inclusive impedindo o processamento da reconstrução do nosso dataset. Outro problema encontrado é a falta de uma forma que seja possível executar o algorítimo ao vivo, ao contrário do \textit{LSD-SLAM} apresentado na próxima seção.


\section{\textit{LSD-SLAM}}

A abordagem seguinte adotada no projeto foi a reconstrução de ambientes utilizando o \textit{LSD-SLAM}. 	O \textit{LSD-SLAM} é uma ferramenta poderosa porém com pouca documentação, o uso dela se mostra trabalhoso devido ao número grande de parâmetros de configuração possíveis e condições de luz e posição, cada ambiente possui suas próprias configurações otimizadas. A capacidade dessa ferramenta de exportar seus mapas 3D em um tipo de arquivo de fácil leitura faz dela facilmente adaptada para qualquer projeto de navegação 3D e reconhecimento espacial. Aprimoramentos em usabilidade e documentação são desejadas para futuras versões dessa ferramenta caso seus desenvolvedores continuem atualizando. Nessa sessão serão discutidos particularidades de configuração, uso, funcionamento e métodos da utilização dessa ferramenta.

\subsection{Configurações utilizadas}

O \textit{LSD-SLAM} foi testado nas seguintes configurações de máquina, um desktop e um laptop, respectivamente :


Processador	AMD FX(tm)-8350 Eight-Core Processor, 4000 Mhz, 4 Núcleos, 8 Processadores Lógicos
8GB de memória RAM;
Placa gráfica AMD Radeon R2 260x;
e:
Processador Intel® Core(™) i3 M370 @ 2.40GHz, 2399 Mhz, 2 Núcleos, 4 Processadores Lógicos
4GB de memória RAM;
Placa gráfica integrada : Intel® HD Graphics.

Softwares e periféricos utilizados:

Ubuntu versão 14.04[21];
Câmera PSEye®, modelo para o console PlayStation® 3;
Câmera embutida no smartphone Motorola® Moto X Play 32GB em junção com o aplicativo OpenCamera para \textit{Android}.
Software VLC para extrair os quadros dos vídeos capturados pela câmera;
Software ROS Indigo [3];
Driver para webcam usb\_cam [5].

No começo da utilização do \textit{LSD-SLAM} foi testada a câmera Logitech® HD Webcam c270, porém essa câmera não oferecia a capacidade mínima recomendado pelo \textit{LSD-SLAM} para obter resultados decentes, que era ter taxa de quadros de pelo menos 30 FPS e global shutter, em que todos os pixels do quadro são expostos ao mesmo tempo, a PSEye® se enquadrava em ambos os requisitos e foi usada no lugar da Logitech. No entanto usar a câmera PSEye® apesar de permitir um resultado melhor, era uma câmera USB que devido a mobilidade da câmera estava limitada ao fio além do próprio tamanho e peso do notebook limitava a captura de datasets a ser feita usando um notebook com a PSEye® conectada. Após algum tempo usando a PSEye®, os testes foram migrados para a câmera do smartphone Motorola® Moto X Play, que tem como diferencial a sua captura de vídeos em mais de 30FPS em alta taxa de bits. Apesar da câmera não ser global shutter e sim rolling shutter, em que os pixels são expostos sequencialmente em uma direção, que é mais comum entre câmeras smartphone, o resultado for satisfatório para o objetivo do trabalho de oferecer o mapeamento 3D do ambiente a partir de um aparelho móvel. A calibração da câmera smartphone foi obtida a partir da calibração do \textit{PhotoGuide} e usada como parâmetro para o \textit{LSD-SLAM} usando o método de conversão na sessão 3.2.3.7.

\subsection{Instalação da estação de trabalho}

Apesar dos detalhes dessa etapa não fazerem parte do escopo desse trabalho, algumas informações técnicas podem auxiliar o leitor a experimentar a ferramenta.

Etapas para preparação:

Instalação do Ubuntu: A instalação do Ubuntu deve ser da versão 12 ou 14 obrigatoriamente, a versão usada no trabalho foi a 14.04;
Instalação do  ROS: O ROS vai ser utilizado para a leitura dos quadros da câmera/dataset além de ser onde a ferramenta é implementada.[3]
Instalação do \textit{LSD-SLAM}[4]
Instalação do  usb\_cam: usb\_cam é um driver ROS para habilitar câmeras USB a serem utilizadas como stream de dados para o ROS e consequentemente o \textit{LSD-SLAM}, se não há intenção de usar câmeras, como no caso da utilização de um dataset, o driver USB não é necessário.
Calibração da câmera: Será detalhado nas próximas sessões.

\subsection{Calibração da câmera}

Uma calibração da câmera do OpenCV pode ser usada no \textit{LSD-SLAM} no entanto ele vem na sua instalação uma ferramenta de calibração mais intuitiva e confiável por oferecer um retorno ao usuário se as amostras tiradas são o suficiente ou não. Além disso câmeras USB não podem ser configuradas usando o software \textit{Android} \textit{PhotoGuide}.

\subsubsection{Imprimindo o padrão tabuleiro de xadrez}

Antes de utilizar a câmera deve-se calibrá-la a fim de eliminar a distorção radial que possa ocorrer ao se capturar quadros da mesma forma que no OpenCV.
Primeiramente deve-se imprimir um tabuleiro de xadrez [7], preferencialmente em uma folha ou cartolina A3 ou A4 e fixado em uma superfície rígida e plana. Essa folha não deve: 

Estar amassada;
Coberta com algum material refletor ou fita adesiva em qualquer parte do tabuleiro;
Estar com alguma “casa” do tabuleiro cortada por impressão;
Estar com tinta esmaecida ou borrada.

Todos esses fatores interferem no reconhecimento do padrão fazendo com que o resultado da calibração se torne falho. Com o tabuleiro em mãos, tomando cuidado para não cobrir as “casas” do tabuleiro com elas, mova-se para um lugar espaçoso de pelo menos 5 $m^2$, livre de obstruções e bem iluminado.

\subsubsection{Compilando e construindo a ferramenta de calibração}

Começa-se obtendo as dependências necessárias e compilando o driver usando os seguintes comandos respectivamente:

\$ rosdep install camera\_calibration
\$ rosmake camera\_calibration
Tabela 3.1: Comandos para inicializar a instalação do módulo que fará a calibração da câmera.

\subsubsection{Envio de frames pela câmera}

Ao final da sessão 5, a câmera deve estar configurada e funcionando. Para ter certeza que a câmera está de fato enviando seus dados para o ROS, usa-se o seguinte comando:

\$ rostopic list
Tabela 3.2: Comandos para inicializar a instalação do módulo que fará a calibração da câmera.
Esse comando irá listar todos os topics que são os nodos ativos no ROS. A saída deve conter esses topics:

%/usb\\_cam/camera\_info
%/usb\_cam/image\_raw
Tabela 3.3: Resultado com os topics.
Caso a saída não apresente erros, a câmera está pronta para ser calibrada.

\subsubsection{Rodando o nodo de calibração}

Para começar a calibração é necessário carregar os topics com as imagens da câmera que será calibrada usando o seguinte comando:

\$ rosrun camera\_calibration cameracalibrator.py --size 9x6 --square 0.108 image:=/usb\_cam/image\_raw camera:=/usb\_cam
Tabela 3.4: Inicialização do módulo que calibrará a câmera.
Vale frisar a importância de que o parâmetro --size é o tamanho do tabuleiro, no entanto ele conta não as casas e sim as linhas entre as casas:

Figura 3.12: Exemplo de padrão utilizado (tabuleiro de xadrez) para a calibração.

Esse padrão por exemplo tem 10x7 casas, no entanto suas linhas são 9x6. O parâmetro --size deve ser nesse caso 9x6.
O parâmetro --square é o tamanho real do lado das casas do tabuleiro que imprimimos em metros. No caso o comando acima indica que cada casa do tabuleiro possui 0,108 metros ou 10,8 centímetros. É recomendado que o usuário que reproduza estes passos use uma régua para medir o seu tabuleiro.
Após executar o comando, a janela de calibração será aberta, como mostra a figura 3.13:

Figura 3.13 - Janela de configuração reconhecendo do tabuleiro

\subsubsection{Movendo o tabuleiro}

Para se obter uma boa calibração é necessário mover o tabuleiro pelo quadro da câmera de modo que:
O tabuleiro se encontre na esquerda, direita, topo e baixo do campo de vista da câmera.
Barra X - Campo de vista esquerda/direita.
Barra Y - Campo de vista topo/baixo.
Barra Size - Aproximando/Afastando e Rotação da câmera
O tabuleiro preencha todo o campo de vista.
O tabuleiro inclinado para a esquerda,direita,topo,baixo (Barra Skew)
A cada passo segure o tabuleiro até que que apareça na imagem o destaque do padrão.





Figuras 3.14-3.19 - Demonstração de posições para calibração
Ao mover o tabuleiro, podem ser percebidas 4 Barras (X,Y,Size,Skew) na barra lateral que aumentam de tamanho conforme ele vai capturando amostras. Quando o botão CALIBRATE estiver iluminado quer dizer que há dados o suficiente para calibrar, ao ser clicado, começará o processo de calibração.
A calibração dura em média um minuto. A janela estará cinza e inativa durante esse tempo, nesse caso a calibração está ocorrendo e o programa não travou, bastando apenas aguardar.

\subsubsection{Resultados da calibração}
Depois de concluída a calibração os resultados dessa calibração podem ser vistos no terminal como essa (os valores poderão diferir com os apresentados, dependendo da câmera utilizada):

D =  [-0.33758562758914146, 0.11161239414304096, -0.00021819272592442094, -3.029195446330518e-05]
K =  [430.21554970319971, 0.0, 306.6913434743704, 0.0, 430.53169252696676, 227.22480030078816, 0.0, 0.0, 1.0]
R =  [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0]
P =  [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0]
%# oST version 5.0 parameters

[image]

width
640

height
480

%[narrow\_stereo/left]

camera matrix
430.215550 0.000000 306.691343
0.000000 430.531693 227.224800
0.000000 0.000000 1.000000

distortion
-0.337586 0.111612 -0.000218 -0.000030 0.0000

rectification
1.000000 0.000000 0.000000
0.000000 1.000000 0.000000
0.000000 0.000000 1.000000

projection
1.000000 0.000000 0.000000 0.000000
0.000000 1.000000 0.000000 0.000000
0.000000 0.000000 1.000000 0.000000


Tabela 3.5: Resultados da calibração impressos no console 
Os valores destacados serão utilizados em breve. Uma calibração bem sucedida resultará em linhas retas na vida real aparecerem como linhas retas na imagem corrigida. Uma calibração falha normalmente resulta em imagens em branco, irreconhecíveis ou que não preservam as linhas retas.
Após uma calibração bem sucedida pode-se ajustar o Deslizador SCALE no topo da janela de calibração para mudar o tamanho da imagem retificada. Uma escala de 0.0 significa que a imagem está formatada de modo que os pixels pretos criados para a preencher os espaços deixados pelos pixels movidos não estejam presentes. A imagem não terá as curvas pretas de correção mas alguns pixels da imagem original serão descartados. A escala de 1.0 significa que todos os pixels da imagem original são visíveis mas a imagem corrigida tem bordas pretas onde não há pixels de entrada da imagem original.
Se a calibração for satisfatória, o botão COMMIT deve ser clicado para enviar os parâmetros de calibração para o armazenamento permanente. A GUI fechará e a seguinte mensagem será impressa no console “writing calibration data to...”.

\subsubsection{Criação do arquivo de calibração}

Do modo que está, o \textit{LSD-SLAM} irá utilizar a calibração salva, no entanto é interessante criar um arquivo de configuração para ser facilmente reutilizado caso se queira usar essa configuração em outra máquina ou salvar a calibração em um local mais seguro. Esse arquivo de calibração também é importante pois o dataset\_slam necessita dele para executar, ou seja, ele não usa a calibração salva pelo calibrador em disco. Na sessão anterior(3.2.3.6) foram destacadas em laranja alguns valores:

width
640

height
480

camera matrix
430.215550 0.000000 306.691343
0.000000 430.531693 227.224800
0.000000 0.000000 1.000000

distortion
-0.337586 0.111612 -0.000218 -0.000030 0.0000
Tabela 3.6: Informações relevantes extraídas da tabela 3.2.
Esses valores serão usados para compor o arquivo. Em um novo documento de texto insira o seguinte modelo:

fx/width fy/height cx/width cy/height d
in\_width in\_height
"crop" / "full" / "none"
out\_width out\_height
Tabela 3.7: Modelo de arquivo de configuração com legenda
Nesse caso deve-se fazer os cálculos substituindo os valores do modelo com os da suas cores respondentes acima, quanto mais precisas as calculadoras usadas melhor. Na terceira linha:
“crop” - Corta a imagem para o tamanho máximo enquanto inclui apenas pixels válidos.
“full” - Não corta a imagem mas pode incluir pixels inválidos.
“none” - Não realiza a operação de correção de distorção radial
Recomenda-se o uso do valor “crop”. O resultado final deve ficar assim de acordo com o o seguinte exemplo:

0.672211796875 0.89694102708333333333333333333333 0.4792052234375 0.473385 -0.337586 0.111612 -0.000218 -0.000030 0.0000 
640 480
crop
640 480
Tabela 3.8: Exemplo de arquivo de configuração final.
Uma observação é que ao inserir os resultados no arquivo, deve-se certificar que o ponto flutuante é ‘.’(ponto) e não ‘,’(vírgula) pois isso fará com que o arquivo não funcione corretamente. Após isso arquivo deve ser salvo como {nome\_do\_arquivo\_de\_calibração.cfg} e estará pronto para ser usado. Se estiver usando um arquivo de calibração do OpenCV, que foi o usado no \textit{PhotoGuide}, a transformação é similar ao resultado da calibração do \textit{LSD-SLAM}. Considerando a seguinte saída da calibração do OpenCV:

<Camera\_Matrix type\_id="opencv-matrix">
<rows>3</rows>
<cols>3</cols>
<dt>d</dt>
<data>
 6.5746697944293521e+002 0. 3.1950000000000000e+002
 0. 6.5746697944293521e+002 2.3950000000000000e+002
 0. 0. 1.
</data></Camera\_Matrix>
<Distortion\_Coefficients type\_id="opencv-matrix">
<rows>5</rows>
<cols>1</cols>
<dt>d</dt>
<data>
 -4.1802327176423804e-001 5.0715244063187526e-001 0. 0. -5.7843597214487474e-001</data></Distortion\_Coefficients>
Tabela 3.9: Exemplo de arquivo de calibração do OpenCV.
Deve ser convertido para um arquivo no formato .cfg de modo que:

fx fy cx cy k1 k2 p1 p2
640 480
"crop" / "full" / "none"
640 480
Tabela 3.10: Modelo de arquivo de configuração usando a calibração do OpenCV.
Lembrando de converter a notação científica, no caso desse exemplo o arquivo final ficará dessa forma:


0.065746697944293521 0.03195 0.065746697944293521 0.02395 -0.41802327176423804 0.50715244063187526 0 0 
640 480
crop
640 480
Tabela 3.11: Exemplo de arquivo de configuração final usando a calibração do OpenCV.

\subsection{Utilização da ferramenta}

Com a câmera calibrada pode-se agora começar a usar de fato a ferramenta, \textit{LSD-SLAM} é dividido em dois pacotes ROS, lsd\_slam\_core e lsd\_slam\_viewer. lsd\_slam\_core contém o sistema SLAM completo, enquanto o lsd\_slam\_viewer é opcionalmente usado para a visualização 3D.
Para impulsionar o sistema do \textit{LSD-SLAM} é o suficiente inicializar um primeiro quadro-chave com uma mapa de profundidade aleatório e alta variância. Com movimentos translacionais da câmera o suficiente nos primeiros segundos o algoritmo “trava” em uma certa configuração, e após algumas propagações de quadros-chaves ela converge para a configuração correta de profundidade.

\subsubsection{lsd\_slam\_viewer - Visualizador 3D}

O visualizador além de ser usado para visualizar também pode ser usado para exportar a nuvem de pontos como .ply. Para operar ao vivo, o seguinte comando é executado:

rosrun lsd\_slam\_viewer viewer
Tabela 3.12: Comando para executar o visualizador 3D.
Esse comando irá abrir a tela de PointCloudViewer, nela é possível ver em tempo real como está a reconstrução do ambiente. Também é possível gravar e reproduzir a saída gerada pelas trajetórias usando respectivamente os comandos para gravar e reproduzir:

rosbag record /lsd\_slam/graph /lsd\_slam/keyframes /lsd\_slam/liveframes -o file\_pc.bag
rosbag play file\_pc.bag
Tabela 3.13: Comandos sequenciais para gravar e reproduzir a saída do visualizador no formato .bag
Não haverá necessidade de reiniciar o visualizador, ele irá reiniciar automaticamente ao se carregar uma entrada diferente. Alguns atalhos úteis para usar na janela:

r : Reset, limpa todos os dados mostrados.
w : Imprime o número de total de pontos, pontos sendo mostrados no momento, Keyframes (Quadros-chave) e restrições no console.
p : Escreve os pontos atualmente mostras como nuvem de pontos para o arquivo: lsd\_slam\_viewer/pc.ply, que pode ser aberto, por exemplo, no Meshlab. Use  em combinação com o sparsityFactor (ver anexo para detalhes) para reduzir o número de pontos escritos.

\subsubsection{Obtendo o mapa 3D usando o live\_slam}

Para obter o mapa ao vivo, usando uma câmera, o seguinte comando deve ser executado:

rosrun lsd\_slam\_core live\_slam /image:=usb\_cam/image\_raw /camera\_info:=usb\_cam
Tabela 3.14: Comando para executar o live\_slam com calibração salva no ROS.
Os parâmetros destacados em vermelhos estão preenchidos com os que o usuário configurou em sua máquina, no caso desse trabalho foi optados pelas nomenclaturas padrões de instalação. Ao se usar esse comando, apenas as dimensões da imagem e a matriz K das mensagens de camera\_info serão usadas, isto é, o vídeo tem que estar corrigido.

Alternativamente, pode-se especificar um arquivo de calibração usando o seguinte comando:

rosrun lsd\_slam\_core live\_slam /image:=usb\_cam/image\_raw \_calib:=/caminho/para/seu/arquivodeccalibracao
Tabela 3.15: Comando para executar o live\_slam com arquivo de calibração externo.
Novamente, o texto em vermelho corresponde ao que foi configurado pelo usuário anteriormente. O texto /caminho/para/seu/arquivodeccalibracao deve ser trocado para o caminho do arquivo de calibração criado.

As telas a seguir mostram exemplos da ferramenta rodando, nota-se que os pontos escritos na nuvem estão invertidos por causa da captura da câmera. 

Figura 3.20 - DebugWindow DEPTH do live\_slam.

Figura 3.21 - PointCloud referente à figura 3.20.


Figura 3.22 - DebugWindow DEPTH do live\_slam, continuação.

Figura 3.23 - PointCloud referente à figura 3.22. continuação.

Figura 3.24 - DebugWindow DEPTH do live\_slam, continuação da execução.

Figura 3.25 - PointCloud referente à figura 3.24, continuação da reconstrução.

O output do lsd\_slam\_core está sendo transferido para o visualizador na janela “PointCloud Viewer” no formato de nuvem de pontos que podem ser rotacionados e observados em qualquer ângulo. A janela DebugWindow DEPTH é a janela que oferece informações sobre a execução do algoritmo incluindo:

Map upd : A taxa em que os quadros estão sendo computados;
Trk : A taxa de quadros do dataset/câmera;
O terceiro parâmetro não possui identificação mas se encontra no formato X/Y/Z e significa rotação do quadro atual com o quadro original nos 3 eixos;
Dens X\% : A densidade média dos pontos;
Good X\% : A quantidade de pontos pontos bons que podem ser utilizados pelo algoritmo, importante para verificar condições de distorção na imagem da câmera como: iluminação, foco, movimentação, taxa de bits insuficiente na compressão ou similaridade daquele quadro atual com os feitos anteriormente;
Scale X\% : Escala entre os pontos
Número de pontos utilizados pelo algoritmo;
Quadro atual;
Numeração de quadro refinador por keyframe.

Além disso as cores representam o grau de proximidade de cada gradiente do quadro. Tomando o verde como referência, quanto mais vermelho mais perto o gradiente está da câmera em relação aos gradientes verdes, nesse caso eles estão entre o verde e a câmera. Em contrapartida quanto mais azul estiver o gradiente mais longe da camera em relação ao gradiente verde está da câmera. Pontos e gradientes brancos foram reconhecidos pelo detecção de gradiente mas não conseguiu ser reconhecido em escala considerando o keyframe atual, normalmente isso ocorre quando há uma diferença de escala inesperada como um objeto que se move ou se a iluminação ou foco da câmera atrapalhar no reconhecimento.

Essas informações são comuns tanto ao LIVE\_SLAM quanto ao DATASET\_SLAM. O método live\_slam se caracteriza por servir de teste do ambiente para se fazer o dataset. Ele possui suas vantagens e desvantagens:

Vantagens:

O funcionamento do algoritmo pode ser visto em tempo real.
A condição de iluminação pode ser testada usando esse modo mais rapidamente.

Desvantagens:

É necessário utilizar em um computador portátil para que possa-se mover com a câmera.
A mobilidade enquanto se usa esse método é prejudicada.
É mais difícil mudar os parâmetros enquanto roda o algoritmo.
Baixa reprodutividade.
Consumo de recursos do computador alto.

Também é possível salvar o vídeo capturado usando o parâmetro do lsd\_slam viewer saveAllVideo, no entanto essa operação é ainda mais custosa, na máquina usada para os testes essa opção não foi possível ser utilizada.

\subsubsection{Obtendo o mapa 3D usando o dataset\_slam}

Para obter o mapa utilizando um dataset, execute o seguinte comando:

rosrun lsd\_slam\_core dataset\_slam \_files:=/caminho/para/seu/dataset \_hz:=<hz> \_calib:=/caminho/para/seu/arquivodeccalibracao
Tabela 3.16: Comando para executar o dataset\_slam com um arquivo de calibração externo
Troque /caminho/para/seu/dataset pelo caminho do seu dataset, <hz> idealmente deverá ser trocado para 0, o que permite o track e mapping sequencial, mas haverá uma queda de performance do que a contrapartida em tempo real. Por fim, troque /caminho/para/seu/arquivodeccalibracao pelo caminho do seu arquivo de configuração resultante na seção 3.2.3.6.
Segue abaixo algumas capturas de tela com o comando sendo executado:

Figuras 3.26 e 3.27 - PointCloud e DebugWindow DEPTH do dataset\_slam.

	Figuras 3.28 e 3.29 - PointCloud e DebugWindow DEPTH do dataset\_slam.

É necessário também, dependendo do dataset usado, um refinamento dos parâmetros de visualização e também da forma que o lsd\_slam\_core opera, pois com os parâmetros default, é possível perceber que a grama em frente ao prédio atrapalhou a captura dos gradientes.

\subsubsection{Parâmetros rqt\_reconfigure}

Uma característica fundamental da ferramenta é a capacidade de modificar alguns parâmetros do algoritmo, esses parâmetros podem ser modificados usando a ferramenta rqt\_reconfigure que vem junto com o pacote do \textit{LSD-SLAM} que pode ser chamado usando o seguinte comando:

rosrun rqt\_reconfigure rqt\_reconfigure
Tabela 3.17 - Comando para inicialização do rqt\_reconfigure

E então será aberta uma janela com vários parâmetros de acordo com quais janelas do \textit{LSD-SLAM} estão abertas, isto é, o lsd\_slam\_core e o viewer. Para cada uma das janelas os parâmetros podem ser modificados em tempo real. A figura 3.30 mostra alguns parâmetros de configuração.

Figura 3.30 - Tela do rqt\_reconfigure

\subsubsubsection{minUseGrad}

Esse parâmetro indica a extensão mínima do gradiente para que o algoritmo o reconheça. Em outras palavras, quanto maior esse valor mais restritivo será a captura de gradientes. diminuindo esse parâmetro, gradientes pequenos como texturas irregulares, grama e outros gradientes não uniformes serão reconhecidos. Deve-se analisar cuidadosamente o quanto do ambiente deseja-se capturar e ajustar esse parâmetro de acordo, assim como o exemplo a seguir:


Figura 3.31 - minUseGrad no seu valor padrão de 5.0

Figura 3.32 - minUseGrad em 25.0

Pelos exemplos dá para perceber que usar o minUseGrad no seu valor padrão de 5.0 acrescenta muito ruido devido à grama, além da própria grama ser uma textura que pode ser facilmente confundida gerando erros de relocalização mais à frente na execução. Ao diminuir o parâmetro, a continuidade do gradiente precisa ser mais uniforme para poder ser aceito pelo algoritmo. Não há heurística para a escolha desse valor e ele deve ser testado para cada ambiente. Também vale frisar que aumentar demais esse valor pode cortar alguns gradientes perfeitamente utilizáveis mas que são menos uniformes como por exemplo a passarela até o prédio na imagem.

\subsubsubsection{KFUsageWeight e KFDistanceWeight}


Esses parâmetros influenciam na frequência em que novos keyframes são obtidos. O KFUsageWeight é em razão da quantidade de quadros usados para refinar, nesse caso um número maior faz com que o algoritmo capture novos keyframes diminuindo a quantidade de refinamento em cada um consequentemente, como no exemplo a seguir:


Figura 3.33 - PointCloud e keyframes do corredor do 1º andar do DCOMP usando o KFUsageWeight padrão de 4.0 e KFDistWeight padrão de 3.0

Figura 3.34 - PointCloud e keyframes do corredor do 1º andar do DCOMP usando o KFUsageWeight máximo de 20.0

Figura 3.35 - PointCloud e KeyFrames do corredor do 1º andar do DCOMP usando o KFUsageWeight máximo de 20.0

Figura 3.36 - PointCloud e KeyFrames do corredor do 1º andar do DCOMP usando o KFUsageWeight máximo de 20.0
Figura 3.37 - PointCloud e KeyFrames do corredor do 1º andar do Dcomp usando o KFUsageWeight máximo de 20.0

Nas janelas do PointCloud, as linhas verdes e vermelhas representam constraints entre keyframes e as pirâmides azuis são keyframes e a posição da câmera em relação ao modelo total. Percebe-se que o valor padrão não reproduziu um resultado tão bom quanto o valor máximo, e também dá para ver que há muito mais constraints e keyframes nas imagem com o valor máximo. Mas apesar de se ter obtido um resultado melhor nesse caso, há uma carga muito maior desse processamento do que pelo valor padrão, fazendo o computador demorar muito mais para executar essa operação devido a quantidade maior de keyframes que o algoritmo tem que rastrear e interligar, a ponto dela não ser bem aplicada no live\_slam, e nem em computadores com potencia reduzida. Da mesma forma o KFDistWeight também modifica a quantidade de keyframes, mas ao contrário do KFUsageWeight, ele faz a partir da distancia, ou seja, caso o dataset possua vários quadros bem próximos fisicamente um do outro e outros quadros mais afastados poderá haver diminuição de keyframes no primeiro caso e o aumento de keyframes no segundo caso. Isso faz com que o refinamento dos keyframes seja mais dependente do dataset fazendo com que seja necessário um cuidado maior na hora de movimentar a câmera para não acabar coletando uma quantidade insuficiente de quadros naquela localização. Segue abaixo o mesmo dataset usado acima só que com o valor máximo do KFDistWeight.

Figura 3.38 - PointCloud e keyframes do corredor do 1º andar do DCOMP usando o KFDistWeight máximo de 20.0

Figura 3.39 - PointCloud e keyframes do corredor do 1º andar do DCOMP usando o KFUsageWeight máximo de 20.0

Como o novo exemplo demonstra, os keyframes se comparados aos do valor padrão são mais numerosos, e o resultado geral também foi melhor. O aumento do parâmetro não é tão custoso para máquina como é no KFUsageWeight, assim  há a possibilidade de usá-lo em computadores com poder de processamento reduzido ou no live\_slam. É importante a experimentação desses parâmetros para se obter o melhor resultado de acordo com o dataset utilizado e a máquina utilizada, para concluir essa sessão, segue um exemplo do valor máximo de ambos os parâmetros juntos.

Figura 3.40 - PointCloud e keyframes do corredor do 1º andar do DCOMP usando o KFUsageWeight e KFDistWeight máximos de 20.0

Figura 3.41 - PointCloud e keyframes do corredor do 1º andar do DCOMP usando o KFUsageWeight e KFDistWeight máximos de 20.0

